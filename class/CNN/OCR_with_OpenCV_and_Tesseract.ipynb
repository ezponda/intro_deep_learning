{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f43016ff",
   "metadata": {},
   "source": [
    "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "<table align=\"center\">\n",
    " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/CNN/OCR_with_OpenCV_and_Tesseract.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/CNN/OCR_with_OpenCV_and_Tesseract.ipynb\">\n",
    "        <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff795efa-14f1-44a6-9d6e-a14f624d5dfe",
   "metadata": {},
   "source": [
    "# Introduction to Optical Character Recognition (OCR)\n",
    "\n",
    "Optical Character Recognition, or OCR, is a technology used to convert different types of documents, such as scanned paper documents, PDF files, or images captured by a digital camera, into editable and searchable data. OCR is widely used for digitizing printed texts so that they can be electronically edited, searched, stored more compactly, displayed online, and used in machine processes such as machine translation, text-to-speech, and data mining.\n",
    "\n",
    "## Challenges in OCR\n",
    "OCR is not always straightforward due to various challenges, such as:\n",
    "\n",
    "1. **Font Variability:** Different font styles and sizes can affect the OCR accuracy.\n",
    "2. **Background Noise:** Images with noisy backgrounds can make text detection difficult.\n",
    "3. **Layout and Formatting:** Complex layouts with columns, boxes, and other formatting elements can complicate text extraction.\n",
    "4. **Image Quality:** Low resolution or blurry images can lead to poor OCR results.\n",
    "\n",
    "## Tesseract OCR\n",
    "\n",
    "Tesseract is an open-source OCR engine. It can read and recognize text in various languages and is widely regarded as one of the most accurate free OCR engines available.\n",
    "\n",
    "### Key Functions of Tesseract\n",
    "\n",
    "1. **Text Detection:** Identifying and extracting textual content from images.\n",
    "2. **Language Support:** Recognizing multiple languages.\n",
    "3. **Customization:** Ability to train Tesseract for new fonts or languages.\n",
    "\n",
    "## Using Tesseract with OpenCV\n",
    "\n",
    "OpenCV (Open Source Computer Vision Library) is an open-source computer vision and machine learning software library. It can be used in conjunction with Tesseract to preprocess images for better OCR results.\n",
    "\n",
    "\n",
    "### Installing Required Libraries\n",
    "\n",
    "\n",
    "#### Install Tesseract OCR \n",
    "\n",
    "First, you need to check if Tesseract OCR is installed on your system.\n",
    "\n",
    "- **For Windows:**\n",
    "  - Download the installer from [Tesseract at UB Mannheim](https://github.com/UB-Mannheim/tesseract/wiki).\n",
    "  - Run the installer and remember the path where you install Tesseract (e.g., `C:\\Program Files\\Tesseract-OCR`).\n",
    "  - You will need this path to set the environment variable.\n",
    "\n",
    "- **For macOS:**\n",
    "  - You can install Tesseract using Homebrew with the command: `brew install tesseract`.\n",
    "\n",
    "- **For Linux:**\n",
    "  - Use the package manager to install Tesseract. For example, on Ubuntu, you can use `sudo apt-get install tesseract-ocr`.\n",
    "\n",
    "#### Set the PATH Environment Variable\n",
    "\n",
    "If Tesseract is installed but not found, you might need to add its installation directory to your system's PATH.\n",
    "\n",
    "- **For Windows:**\n",
    "  - Go to the Control Panel -> System and Security -> System -> Advanced system settings -> Environment Variables.\n",
    "  - Under System Variables, find and select the variable named PATH, then click Edit.\n",
    "  - Add the path to the Tesseract installation directory (e.g., `C:\\Program Files\\Tesseract-OCR`).\n",
    "  - Click OK to save the changes.\n",
    "\n",
    "- **For macOS and Linux:**\n",
    "  - The Tesseract path is usually added to the system PATH automatically. If not, you can add it by modifying the `.bashrc` or `.zshrc` file in your home directory with the command like `export PATH=$PATH:/usr/local/bin/tesseract` (adjust the path as necessary).\n",
    "\n",
    "#### Specify the Path in Your Python Code\n",
    "\n",
    "Alternatively, you can specify the path to the Tesseract executable directly in your Python code:\n",
    "\n",
    "```python\n",
    "import pytesseract\n",
    "\n",
    "# Specify the path to tesseract.exe\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  # Update this path\n",
    "\n",
    "# Rest of your code\n",
    "```\n",
    "\n",
    "Replace `C:\\Program Files\\Tesseract-OCR\\tesseract.exe` with the actual path where Tesseract OCR is installed on your system.\n",
    "\n",
    "\n",
    "```python\n",
    "!pip install opencv-python\n",
    "!pip install pytesseract\n",
    "```\n",
    "\n",
    "### Basic OCR Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f5d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python\n",
    "#!pip install pytesseract\n",
    "#!sudo apt-get install tesseract-ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6439e265-f1b6-4a7a-b7e2-293527f47d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import urllib\n",
    "import pytesseract\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "url = \"https://github.com/ezponda/intro_deep_learning/blob/main/images/restaurant-bar-receipt-sample.jpg?raw=true\"\n",
    "urllib.request.urlretrieve(url, \"restaurant-bar-receipt-sample.jpg\")\n",
    "\n",
    "# Path to the image\n",
    "image_path = \"restaurant-bar-receipt-sample.jpg\"\n",
    "# Load an image\n",
    "image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47117104",
   "metadata": {},
   "source": [
    "##Â `pytesseract.image_to_string` \n",
    "The function takes an image as input and returns the text contained within that image as a string. It's important to preprocess the image to improve OCR accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe50db",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pytesseract.image_to_string(image)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c4506",
   "metadata": {},
   "source": [
    "## Tesseract OCR Options\n",
    "\n",
    "Tesseract provides several command-line options that can be used to optimize its OCR capabilities. These include specifying the tessdata path, language, page segmentation modes (PSM), and OCR Engine modes (OEM), among others. Let's explore some of these options:\n",
    "\n",
    "#### Tessdata Directory\n",
    "- `--tessdata-dir PATH`: Sets the location of the tessdata directory containing language files and other data.\n",
    "\n",
    "#### Language Options\n",
    "- `-l LANG[+LANG]`: Sets the language(s) for OCR. Tesseract supports multiple languages, and you can specify more than one language separated by a plus sign.\n",
    "\n",
    "#### Config Variables\n",
    "- `-c VAR=VALUE`: Sets various configuration variables. You can use multiple `-c` arguments.\n",
    "\n",
    "#### Page Segmentation Modes (PSM)\n",
    "Tesseract offers different page segmentation modes suitable for various image layouts:\n",
    "1. `0`: Orientation and script detection (OSD) only.\n",
    "2. `1`: Automatic page segmentation with OSD.\n",
    "3. `3`: Fully automatic page segmentation, but no OSD. (Default)\n",
    "4. `4`: Assume a single column of text of variable sizes.\n",
    "5. `6`: Assume a single uniform block of text.\n",
    "6. `7`: Treat the image as a single text line.\n",
    "7. `8`: Treat the image as a single word.\n",
    "8. `10`: Treat the image as a single character.\n",
    "9. `11`: Sparse text. Find as much text as possible in no particular order.\n",
    "\n",
    "#### OCR Engine Modes (OEM)\n",
    "Tesseract has different OCR engine modes:\n",
    "1. `0`: Legacy engine only.\n",
    "2. `1`: Neural nets LSTM engine only.\n",
    "3. `2`: Legacy + LSTM engines.\n",
    "4. `3`: Default, based on what is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b603e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tesseract --help-psm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36469da3",
   "metadata": {},
   "source": [
    "### Question 1: Use the correct PSM mode for extracting the tex in the next image\n",
    "\n",
    "```python\n",
    "config = '--psm <PSM mode>'\n",
    "\n",
    "text = pytesseract.image_to_string(image, config=config)\n",
    "```\n",
    "\n",
    "`PSM mode` can be `0, 1, ..., 11`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e9f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/ezponda/intro_deep_learning/blob/main/images/car_plate.png?raw=true'\n",
    "urllib.request.urlretrieve(url, \"car_plate.png\")\n",
    "\n",
    "\n",
    "image_1 = cv2.imread(\"car_plate.png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "plt.imshow(image_1, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3bdbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pytesseract.image_to_string(image_1)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808a9382",
   "metadata": {},
   "source": [
    "### Question 1: Change the `config` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca979bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = '--psm <...> -l <...>'\n",
    "\n",
    "text = pytesseract.image_to_string(image_1, config=config)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dedd24f",
   "metadata": {},
   "source": [
    "\n",
    "### `pytesseract.image_to_boxes` for Text Localization\n",
    "\n",
    "This function returns the recognized characters and their box boundaries, which is useful for understanding how Tesseract is interpreting the text in an image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3659ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import patches\n",
    "\n",
    "\n",
    "# Use pytesseract to detect characters and their boxes\n",
    "config = '--psm 6 -l eng'\n",
    "boxes = pytesseract.image_to_boxes(image, config=config)\n",
    "\n",
    "# Plotting the image and boxes\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(image, cmap='gray')\n",
    "\n",
    "# Adding the boxes to the image\n",
    "for b in boxes.splitlines():\n",
    "    b = b.split(' ')\n",
    "    ax.add_patch(patches.Rectangle((int(b[1]), image.shape[0] - int(b[2])), int(b[3]) - int(b[1]), int(b[2]) - int(b[4]), fill=False, edgecolor='red', lw=2))\n",
    "\n",
    "#plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9097d35-f728-442a-8ba5-f91413a47930",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15480a8f",
   "metadata": {},
   "source": [
    "### `pytesseract.image_to_data`\n",
    "\n",
    "The `pytesseract.image_to_data` function provides a more detailed output compared to image_to_string. It not only extracts text but also gives information about the positioning, confidence scores, and other details for each detected textual element. This function is particularly useful for applications where you need in-depth data about the text layout in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb33cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "config = '--psm 6 -l eng'\n",
    "data = pytesseract.image_to_data(image,config=config, output_type=pytesseract.Output.DICT)\n",
    "print(data.keys())\n",
    "df = pd.DataFrame(data)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169b67c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "# Iterate over each word\n",
    "for i in range(len(df)):\n",
    "    # Only consider entries with a confidence level\n",
    "    if df['conf'][i] > 0:\n",
    "        # Extract coordinates and dimensions of the bounding box\n",
    "        (x, y, w, h) = (df['left'][i], df['top'][i], df['width'][i], df['height'][i])\n",
    "        plt.gca().add_patch(Rectangle((x, y), w, h, fill=False, edgecolor='red', lw=1))\n",
    "        text = f\"{df['text'][i]} ({df['conf'][i]}%)\"\n",
    "        plt.gca().text(x, y - 10, text, color='blue', fontsize=6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418cbf66",
   "metadata": {},
   "source": [
    "### Question 2: Create a script to extract the amount from the bill.\n",
    "\n",
    "Hint: You can use the `line_num` field or `word_num` field from the output DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3ff3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "amount = ''\n",
    "for i, row in df.iterrows():\n",
    "    line_num = row['line_num']\n",
    "    word_num = row['word_num']\n",
    "    text = row['text']\n",
    "    print(line_num, word_num, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b277af2",
   "metadata": {},
   "source": [
    "# Image Preprocessing for Enhancing OCR Quality\n",
    "\n",
    "Image preprocessing is a crucial step in improving the accuracy of Optical Character Recognition (OCR). Preprocessing techniques can help mitigate issues such as noise, distortion, and variable lighting in images, which can significantly impact the performance of OCR algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d1659",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/ezponda/intro_deep_learning/blob/main/images/bad_quality_receipt.png?raw=true'\n",
    "urllib.request.urlretrieve(url, \"bad_quality_receipt.png\")\n",
    "\n",
    "image_bad = cv2.imread('bad_quality_receipt.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "plt.imshow(image_bad, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f8f8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = '--psm 6 -l eng'\n",
    "\n",
    "text = pytesseract.image_to_string(image_bad, config=config)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5b98fd",
   "metadata": {},
   "source": [
    "## Histogram Equalization Using CLAHE\n",
    "\n",
    "Contrast Limited Adaptive Histogram Equalization (CLAHE) is an advanced form of histogram equalization that is used to improve the local contrast of an image. Unlike standard histogram equalization, which applies the same contrast adjustment across the entire image, CLAHE divides the image into smaller blocks and applies histogram equalization to each of these blocks independently. This method can be particularly effective for OCR purposes as it enhances the local contrast and can make the text in an image more readable, especially in cases where the lighting varies across the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b8227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def equalize_histogram(gray_image):\n",
    "    clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(8, 8))\n",
    "    return clahe.apply(gray_image)\n",
    "\n",
    "\n",
    "equalized_image = equalize_histogram(image_bad)\n",
    "\n",
    "# Display both images\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image_bad, cmap='gray')\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(equalized_image, cmap='gray')\n",
    "plt.title('Histogram Equalized Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ee75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = '--psm 6 -l eng'\n",
    "\n",
    "text = pytesseract.image_to_string(equalized_image, config=config)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f041b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from ipywidgets import interact, IntSlider, FloatSlider\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def apply_clahe(clipLimit, tileGridSize):\n",
    "    # Ensure the image is in BGR format\n",
    "    if len(image_bad.shape) == 2 or image_bad.shape[2] == 1:  # Image is already grayscale\n",
    "        gray_image = image_bad\n",
    "    else:\n",
    "        gray_image = cv2.cvtColor(image_bad, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Create a CLAHE object\n",
    "    clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=(tileGridSize, tileGridSize))\n",
    "    \n",
    "    # Apply CLAHE to the grayscale image\n",
    "    clahe_image = clahe.apply(gray_image)\n",
    "    \n",
    "    # Display the results\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(gray_image, cmap='gray')\n",
    "    plt.title('Original Grayscale Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(clahe_image, cmap='gray')\n",
    "    plt.title('CLAHE Enhanced Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Perform OCR on the CLAHE-enhanced image\n",
    "    config = '--psm 6 -l eng'\n",
    "    text = pytesseract.image_to_string(clahe_image, config=config)\n",
    "    # Print the OCR results in a \"cooler\" text box\n",
    "    #print(\"\\033[1;32;40m OCR Result: \\n{}\\033[0m\".format(text))\n",
    "    print(\"OCR Result: \\n{}\".format(text))\n",
    "\n",
    "\n",
    "# Interactive widget for CLAHE parameters\n",
    "interact(apply_clahe, \n",
    "         clipLimit=FloatSlider(min=0, max=5, step=0.1, value=2, description='Clip Limit'),\n",
    "         tileGridSize=IntSlider(min=1, max=32, step=1, value=8, description='Tile Grid Size'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4390f239",
   "metadata": {},
   "source": [
    "## Thresholding\n",
    "\n",
    "Thresholding can be used to create a binary image from a grayscale image. It enhances the contrast between the text and the background, which is often beneficial for OCR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f2c184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_thresholding(image):\n",
    "    return cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "# Apply thresholding\n",
    "thresh_image = apply_thresholding(image_bad)\n",
    "\n",
    "# Display images\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image_bad, cmap='gray')\n",
    "plt.title('Grayscale Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(thresh_image, cmap='gray')\n",
    "plt.title('Thresholded Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a8708",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = '--psm 6 -l eng'\n",
    "\n",
    "text = pytesseract.image_to_string(thresh_image, config=config)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f02eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, widgets\n",
    "\n",
    "\n",
    "def interactive_ocr_preprocessing(clipLimit, tileGridSize, thresh_type, threshold, block_size, C):\n",
    "    # Ensure the image is in BGR format\n",
    "    if len(image_bad.shape) == 2 or image_bad.shape[2] == 1:  # Image is already grayscale\n",
    "        gray_image = image_bad\n",
    "    else:\n",
    "        gray_image = cv2.cvtColor(image_bad, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply CLAHE to the grayscale image, if selected\n",
    "    if clipLimit > 0 and tileGridSize > 0:\n",
    "        clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=(tileGridSize, tileGridSize))\n",
    "        processed_image = clahe.apply(gray_image)\n",
    "    else:\n",
    "        processed_image = gray_image\n",
    "\n",
    "    # Apply thresholding based on the user's choice\n",
    "    if thresh_type == 'Binary':\n",
    "        _, processed_image = cv2.threshold(processed_image, threshold, 255, cv2.THRESH_BINARY)\n",
    "    elif thresh_type == 'Adaptive Mean':\n",
    "        processed_image = cv2.adaptiveThreshold(processed_image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, block_size, C)\n",
    "    elif thresh_type == 'Adaptive Gaussian':\n",
    "        processed_image = cv2.adaptiveThreshold(processed_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, block_size, C)\n",
    "    \n",
    "    # Display the processed image\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(gray_image, cmap='gray')\n",
    "    plt.title('Original Grayscale Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(processed_image, cmap='gray')\n",
    "    plt.title('Processed Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Perform OCR on the processed image\n",
    "    config = '--psm 6 -l eng'\n",
    "    text = pytesseract.image_to_string(processed_image, config=config)\n",
    "    # Print the OCR results in a styled format\n",
    "    print(\"OCR Result: \\n{}\".format(text))\n",
    "\n",
    "interact(interactive_ocr_preprocessing, \n",
    "         clipLimit=widgets.FloatSlider(min=-1, max=5, step=0.1, value=0, description='CLAHE CL', continuous_update=False),\n",
    "         tileGridSize=widgets.IntSlider(min=0, max=16, step=1, value=0, description='CLAHE Grid-Size', continuous_update=False),\n",
    "         thresh_type=widgets.Dropdown(options=['None', 'Binary', 'Adaptive Mean', 'Adaptive Gaussian'], value='None', description='Threshold Type'),\n",
    "         threshold=widgets.IntSlider(min=0, max=255, step=1, value=127, description='Binary Th', continuous_update=False),\n",
    "         block_size=widgets.IntSlider(min=3, max=21, step=1, value=11, description='Adpt Block Size', continuous_update=False, disabled=False),\n",
    "         C=widgets.IntSlider(min=0, max=20, step=1, value=2, description='Adpt C Value', continuous_update=False)\n",
    "        );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cfa0fd",
   "metadata": {},
   "source": [
    "## Noise Removal\n",
    "\n",
    "Removing noise from the image can help in reducing the OCR errors, especially when dealing with low-quality scans or photographs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb32cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(image):\n",
    "    return cv2.medianBlur(image, 3)\n",
    "\n",
    "# Apply noise removal\n",
    "denoised_image = remove_noise(image_bad)\n",
    "\n",
    "# Display images\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image_bad, cmap='gray')\n",
    "plt.title('Thresholded Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(denoised_image, cmap='gray')\n",
    "plt.title('Denoised Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d721ad",
   "metadata": {},
   "source": [
    "## Morphological Operations \n",
    "\n",
    "Morphological operations are a set of operations that process images based on shapes. They apply a structuring element to an input image and produce an output image. The primary operations are dilation and erosion, which can be combined to form operations like opening and closing. These operations are particularly useful in the preprocessing stage of OCR to enhance the visibility of text.\n",
    "\n",
    "### Structuring Element\n",
    "\n",
    "The structuring element is a matrix that decides the nature of the operation being applied to the image. The shape and size of the structuring element affect the outcome of the morphological operation. Common shapes include rectangles, ellipses, and crosses. In OpenCV, `cv2.getStructuringElement()` is used to create a structuring element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37482b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "# This code creates a 5x5 rectangular structuring element.\n",
    "print(kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87239715",
   "metadata": {},
   "source": [
    "### Dilation\n",
    "\n",
    "Dilation adds pixels to the boundaries of objects in an image. It can be used to increase the size of foreground objects.\n",
    "\n",
    "**Code Example:**\n",
    "\n",
    "```python\n",
    "dilated_image = cv2.dilate(src_image, kernel, iterations=1)\n",
    "```\n",
    "\n",
    "Here, `src_image` is the source image, `kernel` is the structuring element, and `iterations=1` indicates that the dilation operation should be applied once.\n",
    "\n",
    "### Erosion\n",
    "\n",
    "Erosion removes pixels at the boundaries of objects. It is used to diminish the features of an image.\n",
    "\n",
    "**Code Example:**\n",
    "\n",
    "```python\n",
    "eroded_image = cv2.erode(src_image, kernel, iterations=1)\n",
    "```\n",
    "\n",
    "### Opening and Closing\n",
    "\n",
    "- **Opening** is erosion followed by dilation. It is useful in removing noise.\n",
    "- **Closing** is dilation followed by erosion. It is useful in closing small holes or gaps in the foreground.\n",
    "\n",
    "**Code Example for Opening:**\n",
    "\n",
    "```python\n",
    "opening_image = cv2.morphologyEx(src_image, cv2.MORPH_OPEN, kernel)\n",
    "```\n",
    "\n",
    "**Code Example for Closing:**\n",
    "\n",
    "```python\n",
    "closing_image = cv2.morphologyEx(src_image, cv2.MORPH_CLOSE, kernel)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cfb72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dilation\n",
    "dilated_image = cv2.dilate(equalized_image, kernel, iterations=1)\n",
    "\n",
    "# Apply erosion\n",
    "eroded_image = cv2.erode(equalized_image, kernel, iterations=1)\n",
    "\n",
    "# Apply opening\n",
    "opening_image = cv2.morphologyEx(equalized_image, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "# Apply closing\n",
    "closing_image = cv2.morphologyEx(equalized_image, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "# Plotting the images\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 5))\n",
    "axs[0].imshow(equalized_image, cmap='gray')\n",
    "axs[0].set_title('equalized_image')\n",
    "axs[1].imshow(dilated_image, cmap='gray')\n",
    "axs[1].set_title('Dilated')\n",
    "axs[2].imshow(eroded_image, cmap='gray')\n",
    "axs[2].set_title('Eroded')\n",
    "axs[3].imshow(opening_image, cmap='gray')\n",
    "axs[3].set_title('Opening')\n",
    "axs[4].imshow(closing_image, cmap='gray')\n",
    "axs[4].set_title('Closing')\n",
    "\n",
    "for ax in axs:\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67bf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dilation\n",
    "dilated_image = cv2.dilate(image_bad, kernel, iterations=1)\n",
    "\n",
    "# Apply erosion\n",
    "eroded_image = cv2.erode(image_bad, kernel, iterations=1)\n",
    "\n",
    "# Apply opening\n",
    "opening_image = cv2.morphologyEx(image_bad, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "# Apply closing\n",
    "closing_image = cv2.morphologyEx(image_bad, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "# Plotting the images\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 5))\n",
    "axs[0].imshow(image_bad, cmap='gray')\n",
    "axs[0].set_title('Original')\n",
    "axs[1].imshow(dilated_image, cmap='gray')\n",
    "axs[1].set_title('Dilated')\n",
    "axs[2].imshow(eroded_image, cmap='gray')\n",
    "axs[2].set_title('Eroded')\n",
    "axs[3].imshow(opening_image, cmap='gray')\n",
    "axs[3].set_title('Opening')\n",
    "axs[4].imshow(closing_image, cmap='gray')\n",
    "axs[4].set_title('Closing')\n",
    "\n",
    "for ax in axs:\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b64b2c9",
   "metadata": {},
   "source": [
    "## Question 3: Use image processing techniques for obtain a better OCR result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c42143c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
