{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1510dee5",
   "metadata": {},
   "source": [
    "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "<table align=\"center\">\n",
    " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/CNN/Object_Detection_YOLO_ultralytics.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/CNN/Object_Detection_YOLO_ultralytics.ipynb\">\n",
    "        <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41964877",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "\n",
    "* [Introduction to Ultralytics](#Introduction-to-Ultralytics)\n",
    "* [Object Detection with Ultralytics using Pretrained YOLO V8 Models](#Object-Detection-with-Ultralytics-using-Pretrained-YOLO-V8-Models)\n",
    "* [Non-Max Suppression (NMS) and Intersection Over Union (IoU)](#Non-Max-Suppression-(NMS)-and-Intersection-Over-Union-(IoU))\n",
    "* [Webcam Local](#Web-cam-Local)\n",
    "* [Different Vision Tasks](#YOLO-for-Different-Vision-Tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04dfebb",
   "metadata": {},
   "source": [
    "# Introduction to [Ultralytics](https://www.ultralytics.com/)\n",
    "\n",
    "Ultralytics is a company that offers a variety of AI-based solutions, including the popular YOLO (You Only Look Once) object detection models. Their YOLO models are known for their speed and accuracy, making them suitable for real-time object detection tasks.\n",
    "\n",
    "The Ultralytics framework provides a convenient and powerful platform for training, evaluating, and deploying object detection models. It is built on top of PyTorch and supports various YOLO versions.\n",
    "\n",
    "For more detailed information and resources on Ultralytics, you can visit the [Ultralytics official documentation](https://docs.ultralytics.com/).\n",
    "\n",
    "\n",
    "\n",
    "You can install it with:\n",
    "\n",
    "```python\n",
    "%pip install torch torchvision torchaudio\n",
    "%pip install ultralytics\n",
    "%pip install opencv-python\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0c8c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torch torchvision torchaudio\n",
    "#%pip install ultralytics\n",
    "#%pip install opencv-python\n",
    "# For using the webcam:\n",
    "#%pip install lap\n",
    "#%pip install lapx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ff36cf",
   "metadata": {},
   "source": [
    "# Object Detection with Ultralytics using Pretrained YOLO Models\n",
    "\n",
    "\n",
    "## Introduction to Object Detection\n",
    "\n",
    "Object detection is a crucial area in computer vision, aiming to recognize and locate objects within images. It has vast applications in fields like autonomous driving, surveillance, and image retrieval.\n",
    "\n",
    "## Introduction to YOLO (You Only Look Once)\n",
    "\n",
    "YOLO is a state-of-the-art, real-time object detection system that applies a single neural network to the full image. This approach divides the image into regions and predicts bounding boxes and probabilities for each region. YOLO is known for its speed and accuracy, making it a popular choice for real-time applications.\n",
    "\n",
    "\n",
    "YOLO (You Only Look Once) is a groundbreaking approach in object detection for its unique way of processing images. Traditional object detection methods apply the detection algorithm multiple times to different parts of the image, whereas YOLO applies a single neural network to the entire image. This network divides the image into a grid and predicts bounding boxes and class probabilities for each grid cell.\n",
    "\n",
    "The advantages of YOLO include:\n",
    "- **Speed**: By processing the entire image in one evaluation, YOLO significantly reduces computational load, enabling real-time detection.\n",
    "- **Accuracy**: Despite its speed, YOLO achieves high accuracy, particularly in detecting small objects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cee767",
   "metadata": {},
   "source": [
    "\n",
    "## Choosing the Pretrained YOLO Model\n",
    "\n",
    "When selecting a pretrained model for your object detection tasks, it's essential to consider the balance between speed and accuracy. The YOLO (You Only Look Once) V8 models come in different sizes to cater to a variety of requirements, from real-time applications to more accuracy-intensive tasks. These models have been trained on the COCO dataset, which is a benchmark in the object detection field.\n",
    "\n",
    "### [COCO (Common Objects in Context)](https://cocodataset.org/#home)\n",
    "\n",
    "- **Dataset Overview**: The COCO dataset is a comprehensive collection for object detection, segmentation, and captioning. It features over 200,000 images and 80 object categories, making it one of the most diverse datasets available. This extensive variety allows for the development and testing of robust object detection models like YOLO V8.\n",
    "\n",
    "\n",
    "### YOLO V8 Model Variants\n",
    "\n",
    "The following table describes the different variants of the YOLO V8 model, providing a comparison based on size, accuracy, speed, and computational requirements:\n",
    "\n",
    "| Model  | Size (pixels) | mAP<sub>val</sub> 50-95 | Speed (CPU ONNX, ms) | Speed (A100 TensorRT, ms) | Parameters (M) | FLOPs (B) |\n",
    "|--------|---------------|------------------------|----------------------|---------------------------|----------------|-----------|\n",
    "| yolov8n | 640          | 37.3                   | 80.4                 | 0.99                      | 3.2            | 8.7       |\n",
    "| yolov8s | 640          | 44.9                   | 128.4                | 1.20                      | 11.2           | 28.6      |\n",
    "| yolov8m | 640          | 50.2                   | 234.7                | 1.83                      | 25.9           | 78.9      |\n",
    "| yolov8l | 640          | 52.9                   | 375.2                | 2.39                      | 43.7           | 165.2     |\n",
    "| yolov8x | 640          | 53.9                   | 479.1                | 3.53                      | 68.2           | 257.8     |\n",
    "\n",
    "\n",
    "### YOLO V9 Model Variants\n",
    "\n",
    "The following table describes the different variants of the YOLO V9 model, providing a comparison based on size, accuracy, and computational requirements:\n",
    "\n",
    "| Model   | Size (pixels) | AP<sub>val</sub> 50-95 | AP<sub>val</sub> 50 | AP<sub>val</sub> 75 | Parameters (M) | FLOPs (B) |\n",
    "|---------|---------------|-----------------------|---------------------|---------------------|----------------|-----------|\n",
    "| YOLOv9-S | 640          | 46.8                  | 63.4                | 50.7                | 7.2            | 26.7      |\n",
    "| YOLOv9-M | 640          | 51.4                  | 68.1                | 56.1                | 20.1           | 76.8      |\n",
    "| YOLOv9-C | 640          | 53.0                  | 70.2                | 57.8                | 25.5           | 102.8     |\n",
    "| YOLOv9-E | 640          | 55.6                  | 72.8                | 60.6                | 58.1           | 192.5     |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **Size (pixels)**: The input resolution for the model. All models use the same input resolution but differ in their internal architecture and complexity.\n",
    "- **mAP<sub>val</sub> 50-95**: The mean Average Precision on the COCO validation dataset, covering IoU thresholds from 0.5 to 0.95. Higher values indicate better accuracy.\n",
    "- **Speed (CPU ONNX, ms/A100 TensorRT, ms)**: Inference speed measured in milliseconds. Lower times indicate faster performance. The speed is provided for both CPU (using ONNX) and NVIDIA A100 GPU (using TensorRT).\n",
    "- **Parameters (M)**: The number of trainable parameters in millions. More parameters typically mean a more complex model that can capture detailed features but may be slower and more memory-intensive.\n",
    "- **FLOPs (B)**: Floating Point Operations per second in billions. This metric gives an idea of the computational demand of the model. Higher values indicate more computational complexity.\n",
    "\n",
    "When choosing a model, consider the trade-off between speed and accuracy that best fits your application's requirements. Smaller models like YOLOv8n are faster but less accurate, suitable for real-time applications. In contrast, larger models like YOLOv8x provide higher accuracy at the cost of increased inference time, suitable for high-accuracy requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592cb457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "model = YOLO('yolov8n.pt')  # load a pretrained YOLOv8n detection, v9 model:  YOLO('yolov9c.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d90dfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826b4754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import urllib\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "url = 'https://akm-img-a-in.tosshub.com/indiatoday/images/story/201812/dogs_and_cats.jpeg?TAxD19DTCFE7WiSYLUdTu446cfW4AbuW&size=770:433'\n",
    "image_path = \"dog-cat.jpg\"\n",
    "urllib.request.urlretrieve(url, image_path)\n",
    "\n",
    "# Read the image in color mode\n",
    "image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Transform to RGB\n",
    "\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bdab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(image)\n",
    "print('Results:\\n')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3cb97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c866ad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[0].boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1597fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in results:\n",
    "    # Extract the original and annotated images\n",
    "    original_img = r.orig_img[..., ::-1]\n",
    "    annotated_image_bgr = r.plot()  # BGR numpy array of predictions\n",
    "    annotated_image_rgb = annotated_image_bgr[..., ::-1]  # Convert BGR to RGB\n",
    "    \n",
    "    # Plot the annotated image\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    plt.imshow(annotated_image_rgb) # RGB PIL image\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26716986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def download_images(image_urls, plot_images=False):\n",
    "    \"\"\"\n",
    "    Downloads images from the given URLs, converts them to RGB format, and optionally plots them.\n",
    "    \n",
    "    Args:\n",
    "    image_urls (list of tuples): A list where each tuple contains the image URL and the desired local file path.\n",
    "    plot_images (bool): If True, the images will be plotted. Defaults to False.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of paths where the images have been saved.\n",
    "    \"\"\"\n",
    "    image_paths = []  # Store the local file paths of the images\n",
    "    \n",
    "    for image_url, image_path in image_urls:\n",
    "        # Download the image from the URL and save it to the local file path\n",
    "        urllib.request.urlretrieve(image_url, image_path)\n",
    "        image_paths.append(image_path)  # Add the local file path to the list\n",
    "        \n",
    "        if plot_images:\n",
    "            # Read the image in color mode\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "            # Convert the image from BGR to RGB format\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Plot the image\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            plt.imshow(image_rgb)  # Display the image in RGB format\n",
    "            plt.axis('off')  # Turn off axis labels and ticks\n",
    "            plt.title(image_path)  # Set the title of the plot as the image path\n",
    "            plt.show()  # Display the plot\n",
    "    \n",
    "    return image_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411dd125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ee50cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_urls = [\n",
    "    ('https://i.ibb.co/R7pRTLy/beach-no-axis.png', 'beach.jpg'),\n",
    "    ('https://i.ibb.co/jL1kZRF/phones.png', 'phones.jpg'),\n",
    "    ('https://i.ytimg.com/vi/1ZupwFOhjl4/maxresdefault.jpg', 'traffic.jpg')\n",
    "]\n",
    "\n",
    "image_paths = download_images(image_urls, plot_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0762b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "results = model(image_paths)\n",
    "\n",
    "# Show results\n",
    "for i, r in enumerate(results):\n",
    "    # Extract the original and annotated images\n",
    "    original_img = r.orig_img[..., ::-1]\n",
    "    annotated_image_bgr = r.plot()  # BGR numpy array of predictions\n",
    "    annotated_image_rgb = annotated_image_bgr[..., ::-1]  # Convert BGR to RGB\n",
    "    \n",
    "    # Plot the annotated image\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(annotated_image_rgb) # RGB PIL image\n",
    "    plt.axis('off')\n",
    "    plt.title(image_paths[i])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc9727d",
   "metadata": {},
   "source": [
    "### Compare different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bac247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def compare_models(image_path: str, model_names: List[str], conf_threshold: float=0.25):\n",
    "    \"\"\"\n",
    "    Simple function to compare different YOLO models on the same image.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the image\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Set up plot\n",
    "    fig, axs = plt.subplots(len(model_names), 1, figsize=(10, 5 * len(model_names)))\n",
    "    if len(model_names) == 1:\n",
    "        axs = [axs]  # Make it iterable if only one model\n",
    "    \n",
    "    # Print summary header\n",
    "    print(\"Model Comparison Summary:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Model':<12} | {'Detections':<10} | {'Inference Time (ms)':<20}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Process each model\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        # Load model\n",
    "        model = YOLO(model_name)\n",
    "        \n",
    "        # Measure inference time\n",
    "        start_time = time.time()\n",
    "        results = model(img_rgb, conf=conf_threshold)\n",
    "        inference_time = (time.time() - start_time) * 1000  # ms\n",
    "        \n",
    "        # Get detection count\n",
    "        num_detections = len(results[0].boxes)\n",
    "        \n",
    "        # Plot results\n",
    "        axs[i].imshow(results[0].plot())\n",
    "        axs[i].set_title(f\"{model_name}: {num_detections} objects, {inference_time:.1f}ms\", fontsize=12)\n",
    "        axs[i].axis('off')\n",
    "        \n",
    "        # Print summary line\n",
    "        print(f\"{model_name:<12} | {num_detections:<10} | {inference_time:.1f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_models('traffic.jpg', [\"yolov8n.pt\", \"yolov10n.pt\", \"yolo11n.pt\", \"yolo12l.pt\"], conf_threshold=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2f8445",
   "metadata": {},
   "source": [
    "### Non-Max Suppression (NMS) and Intersection Over Union (IoU) \n",
    "\n",
    "Object detection models, often detect multiple bounding boxes around the same object. This leads to redundant detections for the same object, which is undesirable. To resolve this, two key concepts are utilized: Intersection Over Union (IoU) and Non-Max Suppression (NMS). These techniques help refine the boxes surrounding detected objects, ensuring each object is identified accurately and uniquely.\n",
    "\n",
    "#### Intersection Over Union (IoU)\n",
    "\n",
    "IoU is a metric used to quantify the percent overlap between two bounding boxes. It is calculated by dividing the area of overlap between the two boxes by the area of their union:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{IoU} = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}}\n",
    "\\end{equation*}\n",
    "\n",
    "For object detection, IoU is utilized to determine how close a predicted bounding box is to the ground truth bounding box. During evaluation, a higher IoU represents a better prediction by the model.\n",
    "\n",
    "However, in the context of running inference with a model, IoU is crucial for Non-Max Suppression.\n",
    "\n",
    "#### Non-Max Suppression (NMS)\n",
    "\n",
    "Non-Max Suppression is a technique to ensure that only the most probable bounding box for an object is preserved while all other redundant boxes are removed. Here’s how it generally works:\n",
    "\n",
    "1. Select the box with the highest probability of object detection (confidence score).\n",
    "2. Compute the IoU of this box with all other boxes. If any box has an IoU greater than a set threshold (typically between 0.5 and 0.7), it is suppressed (i.e., removed).\n",
    "3. Repeat this process for all boxes until each detected object is represented by only one box.\n",
    "\n",
    "NMS ensures that in cases where multiple boxes predict the same object, only the most accurate one is kept.\n",
    "\n",
    "#### Usage in Ultralytics YOLO Inference\n",
    "\n",
    "When you run inference using the Ultralytics YOLO model, you can control the behavior of NMS and IoU through the inference arguments:\n",
    "\n",
    "- `conf`: This is the confidence threshold. Detections with a confidence score below this threshold are disregarded before NMS. By adjusting this value, you can filter out weak detections early. **(default 0.25)**\n",
    "- `iou`: This is the IoU threshold for NMS. In areas where multiple bounding boxes overlap, if the overlap (IoU) is greater than this threshold, only the box with the highest confidence is kept. **(default 0.7)**\n",
    "\n",
    "Here is how you can use these parameters in practice:\n",
    "\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pretrained YOLO model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Run inference with custom confidence and IoU thresholds\n",
    "results = model.predict(image, conf=0.25, iou=0.7)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4031c2",
   "metadata": {},
   "source": [
    "#### Question 1: Change confidence and IoU thresholds for detecting more objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce1b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "results = model(image_paths, conf=..., iou=...)\n",
    "\n",
    "# Show results\n",
    "for r in results:\n",
    "    # Extract the original and annotated images\n",
    "    original_img = r.orig_img[..., ::-1]\n",
    "    annotated_image_bgr = r.plot()  # BGR numpy array of predictions\n",
    "    annotated_image_rgb = annotated_image_bgr[..., ::-1]  # Convert BGR to RGB\n",
    "    \n",
    "    # Plot the annotated image\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(annotated_image_rgb) # RGB PIL image\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9e175",
   "metadata": {},
   "source": [
    "## Web cam Local\n",
    "\n",
    "### Detection loop\n",
    "\n",
    "The detection loop consists of four phases:\n",
    "\n",
    "- Loading the webcam frame\n",
    "\n",
    "- Running the image through the model\n",
    "\n",
    "- Updating the output with the resulting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4591d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import Image as IPyImage\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "time.sleep(1)  ### letting the camera autofocus\n",
    "\n",
    "axes = None\n",
    "NUM_FRAMES = 50  # you can change this\n",
    "processed_imgs = []\n",
    "for i in range(NUM_FRAMES):\n",
    "    # Load frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Run the model\n",
    "    result = model(frame, verbose=False)\n",
    "    annotated_image_bgr = result[0].plot()\n",
    "    annotated_image_rgb = annotated_image_bgr[:,:, ::-1]  # Convert BGR to RGB\n",
    "    \n",
    "    img = Image.fromarray(np.uint8(annotated_image_rgb))\n",
    "    processed_imgs.append(img)\n",
    "    cv2.imshow(\"test\", annotated_image_bgr)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73a378f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b26b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create gif\n",
    "processed_imgs[0].save('web_cam.gif',\n",
    "                       format='GIF',\n",
    "                       append_images=processed_imgs[1:],\n",
    "                       save_all=True,\n",
    "                       duration=100,\n",
    "                       loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a741ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPyImage('web_cam.gif', format='png', width=15 * 40, height=3 * 40) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75e0109",
   "metadata": {},
   "source": [
    "## Question 2: Traffic Scene  Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2a7f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the scene gif\n",
    "url_1 = 'https://i.ibb.co/wpHvb58/scene1.gif'\n",
    "scene_1_path = 'scene_1.gif'\n",
    "urllib.request.urlretrieve(url_1, scene_1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc6d88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPyImage(scene_1_path, format='png', width=15 * 40, height=3 * 40)\n",
    "IPyImage(url=url_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d0fc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8x.pt')  # load a pretrained YOLOv8n detection, v9 model:  YOLO('yolov9c.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f510d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "gif_object = Image.open(scene_1_path)\n",
    "\n",
    "# Display individual frames from the loaded animated GIF file\n",
    "processed_imgs = []\n",
    "for _, ind in tqdm(enumerate(range(0, gif_object.n_frames))):\n",
    "    gif_object.seek(ind)\n",
    "    ## frame in numpy array format (512, 512, 3)\n",
    "    frame = np.array(gif_object.convert('RGB'))[:,:,::-1]\n",
    "    \n",
    "    ## Object Detection code\n",
    "    ...\n",
    "    \n",
    "    img = Image.fromarray(np.uint8(annotated_image_rgb))\n",
    "    processed_imgs.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e69143",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the processed scene gif\n",
    "processed_imgs[0].save('scene1_boxes.gif',\n",
    "                       format='GIF',\n",
    "                       append_images=processed_imgs[1:],\n",
    "                       save_all=True,\n",
    "                       duration=200,\n",
    "                       loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe6e5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPyImage('scene1_boxes.gif', format='png', width=15 * 40, height=3 * 40) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ccdab1",
   "metadata": {},
   "source": [
    "## YOLO for Different Vision Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e58be7",
   "metadata": {},
   "source": [
    "###  Segmentation\n",
    "\n",
    "Segmentation models, indicated by the `-seg` suffix (e.g., `yolov8n-seg.pt`), are designed for more detailed analysis. These models not only detect objects but also delineate their exact shapes, segmenting each object from the background and other objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951821bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolo11n-seg.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d3ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cabb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model('traffic.jpg')\n",
    "results[0].show()  # Display the segmentation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f237a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = results[0]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0f7174",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca64d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2351f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.engine.results import Results\n",
    "\n",
    "def extract_segmentation_mask(\n",
    "    detection_result: Results,\n",
    "    object_index: int = 0\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extracts a binary segmentation mask for a specific detected object.\n",
    "    \n",
    "    This function creates a precise binary mask from YOLO segmentation contours\n",
    "    where white pixels (255) represent the object and black pixels (0) represent \n",
    "    the background.\n",
    "    \n",
    "    Parameters:\n",
    "        detection_result: The YOLO Results object containing segmentation data\n",
    "        object_index: The index of the object to extract (0 for first detection)\n",
    "    \n",
    "    Returns:\n",
    "        A binary mask as a NumPy array of type uint8\n",
    "    \"\"\"\n",
    "    # Verify the inputs\n",
    "    if detection_result.masks is None:\n",
    "        raise ValueError(\"No segmentation masks found in detection results\")\n",
    "    \n",
    "    if object_index >= len(detection_result.masks.xy):\n",
    "        raise IndexError(f\"Object index {object_index} is out of range. \"\n",
    "                         f\"Only {len(detection_result.masks.xy)} objects detected.\")\n",
    "    \n",
    "    # Get original image dimensions\n",
    "    height, width = detection_result.orig_img.shape[:2]\n",
    "    \n",
    "    # Create an empty mask with the original image dimensions\n",
    "    binary_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    # Get the contour points from the detection result\n",
    "    contour_points = detection_result.masks.xy[object_index]\n",
    "    \n",
    "    # Format the contour for OpenCV's drawContours function\n",
    "    formatted_contour = contour_points.astype(np.int32).reshape(-1, 1, 2)\n",
    "    \n",
    "    # Draw the filled contour onto the empty mask\n",
    "    cv2.drawContours(binary_mask, [formatted_contour], -1, 255, cv2.FILLED)\n",
    "\n",
    "    # Convert the mask to a binary format\n",
    "    binary_mask = binary_mask.astype(bool)\n",
    "\n",
    "    return binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903ccfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.copy(res.orig_img)\n",
    "\n",
    "for object_index in range(len(res.boxes.cls.tolist())):\n",
    "    if object_index == 2:\n",
    "        break\n",
    "    # Get class label\n",
    "    label = res.names[res.boxes.cls.tolist()[object_index]]\n",
    "    print('_'*50)\n",
    "    print(label)\n",
    "\n",
    "    # Create the binary mask for the object\n",
    "    b_mask = extract_segmentation_mask(res, object_index)\n",
    "\n",
    "    # Create the isolated image from the binary b_mask\n",
    "    masked_img = img.copy()\n",
    "    # Set background to black\n",
    "    masked_img[~b_mask] = 0\n",
    "\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    plt.imshow(masked_img) # RGB PIL image\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1672efa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_image_bgr = res.plot()  # BGR numpy array of predictions\n",
    "annotated_image_rgb = annotated_image_bgr[..., ::-1]  # Convert BGR to RGB\n",
    "\n",
    "# Plot the annotated image\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.imshow(annotated_image_rgb) # RGB PIL image\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b137eda",
   "metadata": {},
   "source": [
    "#### Working with Segmentation Masks: Visualization and Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b4dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://upload.wikimedia.org/wikipedia/commons/d/d3/Albert_Einstein_Head.jpg'\n",
    "image_path = 'einstein.jpg'\n",
    "urllib.request.urlretrieve(url, image_path)\n",
    "\n",
    "# Load and display image\n",
    "image = cv2.imread(image_path)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(image_rgb)\n",
    "plt.axis('off')\n",
    "plt.title('Original Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c9c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run segmentation\n",
    "results = model(image, conf=0.3)  # Lower confidence threshold for our example\n",
    "result = results[0]\n",
    "\n",
    "# Display the default visualization\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(result.plot())\n",
    "plt.axis('off')\n",
    "plt.title('Default YOLO Segmentation Result')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102ef0f2",
   "metadata": {},
   "source": [
    "##### Explore the Segmentation Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb2930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available attributes in result:\")\n",
    "print(dir(result))\n",
    "\n",
    "# Check mask information\n",
    "if result.masks is not None:\n",
    "    print(\"\\nMask information:\")\n",
    "    print(f\"Number of masks: {len(result.masks)}\")\n",
    "    print(f\"Shape of first mask: {result.masks.data[0].shape}\")\n",
    "    print(f\"Data type: {result.masks.data[0].dtype}\")\n",
    "    \n",
    "    # Get class information\n",
    "    if len(result.boxes) > 0:\n",
    "        print(\"\\nDetected classes:\")\n",
    "        for i, box in enumerate(result.boxes):\n",
    "            class_id = int(box.cls)\n",
    "            class_name = result.names[class_id]\n",
    "            confidence = float(box.conf)\n",
    "            print(f\"Object {i+1}: {class_name} (Confidence: {confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8330fc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixelate(image, blocks=16):\n",
    "    \"\"\"Heavily pixelate an image region by downsampling and upsampling\"\"\"\n",
    "    # Get dimensions\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Downsample\n",
    "    temp = cv2.resize(image, (blocks, blocks), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # Upsample back to original size\n",
    "    return cv2.resize(temp, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "# Step 1: Make sure we have a mask for the detected object\n",
    "if result.masks is not None and len(result.masks) > 0:\n",
    "    # Get the mask for the first detected object\n",
    "    object_mask = extract_segmentation_mask(result, 0)\n",
    "    \n",
    "    # Step 2: Create a pixelated version of the entire image\n",
    "    pixelated_image = pixelate(image, blocks=16)\n",
    "    \n",
    "    # Step 3: Create two different pixelation effects\n",
    "    \n",
    "    # Example 1: Pixelate everything EXCEPT the object\n",
    "    inverse_pixelation = pixelated_image.copy()\n",
    "    inverse_pixelation[object_mask] = image_rgb[object_mask]\n",
    "    \n",
    "    # Example 2: Pixelate ONLY the object\n",
    "    object_pixelation = image_rgb.copy()\n",
    "    object_pixelation[object_mask] = pixelated_image[object_mask]\n",
    "    \n",
    "    # Display the results\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.imshow(pixelated_image)\n",
    "    plt.title('Fully Pixelated Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.imshow(inverse_pixelation)\n",
    "    plt.title('Pixelated Background, Clear Object')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.imshow(object_pixelation)\n",
    "    plt.title('Clear Background, Pixelated Object')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a332544",
   "metadata": {},
   "source": [
    "### Question 3: Real-time Person Pixelation with Webcam\n",
    "\n",
    "For this final exercise, create a webcam application that automatically detects and pixelates all people in the frame while keeping the rest of the image clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15100c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import Image as IPyImage\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Load the YOLO segmentation model\n",
    "model = YOLO(...)\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "time.sleep(1)  # Letting the camera autofocus\n",
    "\n",
    "# Parameters\n",
    "NUM_FRAMES = 200  # Number of frames to capture\n",
    "processed_imgs = []\n",
    "\n",
    "\n",
    "# Process webcam feed\n",
    "for i in tqdm(range(NUM_FRAMES)):\n",
    "    # Load frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Make a copy for visualization\n",
    "    display_frame = frame.copy()\n",
    "    \n",
    "    # Run YOLO segmentation\n",
    "    results = model(frame, verbose=False)\n",
    "    result = results[0]\n",
    "    \n",
    "    # Find the person class ID\n",
    "    # Hint: Look through result.names dictionary to find the 'person' class\n",
    "    person_class_id = ...\n",
    "    \n",
    "    # Create pixelated version of the entire frame\n",
    "    pixelated_frame = pixelate(frame, blocks=10)\n",
    "    \n",
    "    # Check if any people were detected\n",
    "    if result.masks is not None and len(result.boxes) > 0:\n",
    "        # Create a combined mask for all people\n",
    "        h, w = frame.shape[:2]\n",
    "        people_mask = np.zeros((h, w), dtype=bool)\n",
    "        \n",
    "        # Process each detected person\n",
    "        for i, box in enumerate(result.boxes):\n",
    "            # Use the correct class ID to identify people\n",
    "            if ...:\n",
    "                # Use the extract_segmentation_mask function\n",
    "                # to get the mask for this person\n",
    "                current_mask = ...\n",
    "                \n",
    "                # Combine with the overall people mask\n",
    "                people_mask = ...\n",
    "        \n",
    "        # Apply pixelation only to people areas\n",
    "        # Use boolean indexing to replace only the masked areas\n",
    "        ...\n",
    "    \n",
    "    # Display the processed frame\n",
    "    cv2.putText(display_frame, \"People Pixelation\", (10, 30), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Privacy Filter\", display_frame)\n",
    "    \n",
    "    # Convert to RGB for PIL and save to list\n",
    "    display_rgb = cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(np.uint8(display_rgb))\n",
    "    processed_imgs.append(img)\n",
    "    \n",
    "    # Exit if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save processed frames as a GIF\n",
    "# Complete the code to save the frames as a GIF\n",
    "if processed_imgs:\n",
    "    ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47250c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the GIF\n",
    "try:\n",
    "    IPyImage('webcam_pixelated_people.gif', format='png', width=15 * 40, height=10 * 40)\n",
    "except:\n",
    "    print(\"GIF display failed. The file should still be saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c7965",
   "metadata": {},
   "source": [
    "### Pose\n",
    "\n",
    "Pose estimation models, identified by the `-pose` suffix (e.g., `yolov8n-pose.pt`), are specialized in detecting human figures and estimating their postures by identifying key body points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ca2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8l-pose.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829840bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_urls = [\n",
    "    (\n",
    "        'https://images.unsplash.com/photo-1561049501-e1f96bdd98fd?q=80&w=2778&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D',\n",
    "        'yoga_1.jpg'\n",
    "    ),\n",
    "    (\n",
    "        'https://images.unsplash.com/photo-1545205597-3d9d02c29597?q=80&w=2940&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D',\n",
    "        'yoga_2.jpg'\n",
    "    ),\n",
    "]\n",
    "image_paths = download_images(image_urls, plot_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dee5bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "results = model(image_paths)\n",
    "\n",
    "# Show results\n",
    "for i, r in enumerate(results):\n",
    "    # Extract the original and annotated images\n",
    "    original_img = r.orig_img[..., ::-1]\n",
    "    annotated_image_bgr = r.plot()  # BGR numpy array of predictions\n",
    "    annotated_image_rgb = annotated_image_bgr[..., ::-1]  # Convert BGR to RGB\n",
    "    \n",
    "    # Plot the annotated image\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(annotated_image_rgb) # RGB PIL image\n",
    "    plt.axis('off')\n",
    "    plt.title(image_paths[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43733cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.keypoints.conf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b495e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.keypoints.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3532523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
