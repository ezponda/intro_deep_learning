{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "<table align=\"center\">\n",
    " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/CNN/Siamese_net.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/CNN/Siamese_net.ipynb\">\n",
    "        <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ke8z2tlIK4iX"
   },
   "source": [
    "We are going to create a siamese net for learning a metric to calculate similarities and dissimilarities between different types of images.\n",
    "\n",
    "<img src=\"https://i.ibb.co/FV4pj65/siamese.png\" alt=\"Alex-Net-architecture\" border=\"0\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hge3dmmA0Rug"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQAPn9sb0Rup"
   },
   "source": [
    "## Simple Siamese-Net with Fashion MNIST\n",
    "\n",
    "Fashion MNIST dataset contains 70,000 grayscale images with 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWas9HgU0Ruq"
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# prepare train and test sets\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "\n",
    "# normalize values\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "\n",
    "class_names = [\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt',\n",
    "    'Sneaker', 'Bag', 'Ankle boot'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-myJBGF0Rus"
   },
   "outputs": [],
   "source": [
    "# We create triplets where the anchor and the positive have the same class.  \n",
    "# The negative has different class\n",
    "def create_triplets(images, labels, n_triplets_per_class=7500):\n",
    "    triplets = [] # (anchor, positive, negative)\n",
    "    n_labels = len(np.unique(labels))\n",
    "    for class_ind in range(n_labels):\n",
    "        # Iterate over the classes\n",
    "        pos_class_inds = np.argwhere(labels == class_ind).flatten()\n",
    "        neg_class_inds = np.argwhere(labels != class_ind).flatten()\n",
    "        for _ in range(n_triplets_per_class):\n",
    "            # anchor\n",
    "            anchor_ind = np.random.choice(pos_class_inds)\n",
    "            anchor = images[anchor_ind]\n",
    "            # positive\n",
    "            pos_ind = np.random.choice(pos_class_inds)\n",
    "            positive = images[pos_ind]\n",
    "            # negative\n",
    "            neg_ind = np.random.choice(neg_class_inds)\n",
    "            negative = images[neg_ind]\n",
    "            #\u00a0add triplet\n",
    "            triplets.append((anchor, positive, negative))\n",
    "    triplets = np.array(triplets)\n",
    "    return triplets\n",
    "\n",
    "train_triplets = create_triplets(train_images, train_labels, 7500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lvg2yKHD0Rus",
    "outputId": "1399f3d4-f559-4c85-d1f2-c2648627b75b"
   },
   "outputs": [],
   "source": [
    "train_triplets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "Vs7XfvHQ0Rux",
    "outputId": "076cc243-4ce5-4310-e156-2e1225a8bd95"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "ind = np.random.randint(len(train_triplets))\n",
    "labels = ['Anchor', 'Positive', 'Negative']\n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_triplets[ind,i,:,:], cmap='gray')\n",
    "    plt.xlabel(labels[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triplet Loss\n",
    "\n",
    "- A is  \"Anchor\" \n",
    "- P is  \"Positive\" \n",
    "- N is  \"Negative\" \n",
    "\n",
    " $(A^{(i)}, P^{(i)}, N^{(i)})$   is the $i$-th training triplet. \n",
    "\n",
    "You'd like to make sure that an image $A^{(i)}$  is closer to the Positive $P^{(i)}$ than to the Negative image $N^{(i)}$) by at least a margin $\\alpha$:\n",
    "\n",
    "$$\\mid \\mid \\mathrm{vec}(A^{(i)}) - \\mathrm{vec}(P^{(i)}) \\mid \\mid_2^2 + \\alpha < \\mid \\mid \\mathrm{vec}(A^{(i)}) - \\mathrm{vec}(N^{(i)}) \\mid \\mid_2^2$$\n",
    "\n",
    "We want to minimize the  \"triplet cost\":\n",
    "\n",
    "$$\\mathcal{J}(A,P,N) = \\sum^{m}_{i=1} \\max \\left(  \\small \\underbrace{\\mid \\mid \\mathrm{vec}(A^{(i)}) - \\mathrm{vec}(P^{(i)}) \\mid \\mid_2^2}_\\text{(1)} - \\underbrace{\\mid \\mid \\mathrm{vec}(A^{(i)}) - \\mathrm{vec}(N^{(i)}) \\mid \\mid_2^2}_\\text{(2)} + \\alpha  , 0 \\right)\u00a0$$\n",
    "\n",
    "- The term (1) is the squared distance between the anchor \"A\" and the positive \"P\" for a given triplet; you want this to be small. \n",
    "```python\n",
    "pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
    "```\n",
    "\n",
    "- The term (2) is the squared distance between the anchor \"A\" and the negative \"N\" for a given triplet, you want this to be relatively large. It has a minus sign preceding it because minimizing the negative of the term is the same as maximizing that term.\n",
    "```python\n",
    "neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n",
    "```\n",
    "\n",
    "- $\\alpha$ is  the margin.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHkeGRXb0Ruz"
   },
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, margin = 0.2):\n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
    "    neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n",
    "    loss = tf.maximum(pos_dist - neg_dist + margin, 0.0)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "def distance(vec1, vec2):\n",
    "    return tf.reduce_sum(tf.square(vec1 - vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8kjjqfL0Ru0"
   },
   "source": [
    "###\u00a0Create Model\n",
    "\n",
    "First of all, we create the shared `base_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NPbQZPfq0Ru1",
    "outputId": "68d82ad7-8246-4b34-9539-bf6840ee8288"
   },
   "outputs": [],
   "source": [
    "vec_dim = 64\n",
    "\n",
    "inputs = tf.keras.Input(shape=(28,28,))\n",
    "x = layers.Flatten(name=\"flatten_input\")(inputs)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.25)(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.25, name=\"second_dropout\")(x)\n",
    "\n",
    "x = layers.Dense(vec_dim, activation='linear')(x)\n",
    "# L2 normalize\n",
    "x = layers.Lambda(lambda z: tf.math.l2_normalize(z,axis=1))(x)\n",
    "\n",
    "base_model = tf.keras.Model(inputs=inputs, outputs=x, name='base_model')\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAcI5U_j0Ru2"
   },
   "outputs": [],
   "source": [
    "#\u00a0Input triplets (anchor, positive, negative)  \n",
    "inputs = tf.keras.Input(shape=(3, 28, 28), name='inputs')\n",
    "\n",
    "## Anchor vector\n",
    "anchor_input = inputs[:, 0, :, :]\n",
    "anchor_vec = base_model(anchor_input)\n",
    "\n",
    "## Positive vector\n",
    "positive_input = inputs[:, 1, :, :]\n",
    "positive_vec = base_model(positive_input)\n",
    "\n",
    "## Negative vector\n",
    "negative_input = inputs[:, 2, :, :]\n",
    "negative_vec = base_model(negative_input)\n",
    "\n",
    "## stack all the vectors dim: (3, none, vec_dim)\n",
    "outputs = tf.stack([anchor_vec, positive_vec, negative_vec])\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "lV_Yh8gK0Ru2",
    "outputId": "b408a88a-8808-442d-d018-deed4725d6d1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HM-LBQTo0Ru2"
   },
   "outputs": [],
   "source": [
    "y = np.zeros(len(train_triplets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PsudWqWL0Ru3",
    "outputId": "88054ffa-609b-4cb8-e709-31f8720bcbd8"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=triplet_loss,optimizer='adam')\n",
    "history = model.fit(train_triplets, y, epochs=3, validation_split=0.15, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDI4Husz0Ru3"
   },
   "source": [
    "### Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qFigwBZj0Ru3",
    "outputId": "f2148828-77fa-421a-b3f0-e9d386ff765f"
   },
   "outputs": [],
   "source": [
    "#(test_images, test_labels)\n",
    "image = test_images[5]\n",
    "image_vec = base_model(np.expand_dims(image, 0))\n",
    "image.shape, image_vec.shape, image_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QkWi8BeY0Ru4",
    "outputId": "f2caff51-002c-4798-c24b-8ba0fde8de2c"
   },
   "outputs": [],
   "source": [
    "def plot_images_dis(base_model, image_vec, images, labels, class_names, margin=0.2):\n",
    "    plt.figure(figsize=(30, 30))\n",
    "    for class_ind in range(len(class_names)):\n",
    "        class_name = class_names[class_ind]\n",
    "        class_inds = np.argwhere(labels == class_ind).flatten()\n",
    "        image_class = np.squeeze(images[np.random.choice(class_inds)])\n",
    "        image_class_vec = base_model(np.expand_dims(image_class, 0))\n",
    "        dis = np.round(distance(image_vec, image_class_vec), 3)\n",
    "        dis_str = str(dis)\n",
    "        if dis < margin:\n",
    "            color = 'green'\n",
    "        else:\n",
    "            color = 'red'\n",
    "        plt.subplot(1, len(class_names), class_ind + 1)\n",
    "        plt.title(class_name, color=color)\n",
    "        plt.xlabel(dis_str, color=color)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(image_class)\n",
    "    plt.show()\n",
    "\n",
    "for class_ind in range(10):\n",
    "    print('-'*80)\n",
    "    class_name = class_names[class_ind]\n",
    "    print(class_name, class_ind)\n",
    "    class_inds = np.argwhere(test_labels == class_ind).flatten()\n",
    "    image = np.squeeze(test_images[np.random.choice(class_inds)])\n",
    "    image_vec = base_model(np.expand_dims(image, 0))\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "    print('distances:')\n",
    "    plot_images_dis(base_model, image_vec, test_images, test_labels, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\u00a0Similarity and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nearest_class(image_vec, image_class, vecs, labels):\n",
    "    '''Find the neares class of a single image vector'''\n",
    "    #\u00a0vectors are normalized: argmin_i |v -v_i| = argmax_i v_i.dot(v)\n",
    "    most_similar_ind = np.argsort(-vecs.dot(image_vec))[1]\n",
    "    # index 0 is for the same vector\n",
    "    nearest_class = labels[most_similar_ind]\n",
    "    return nearest_class\n",
    "\n",
    "\n",
    "def calculate_confusion_matrix(vecs, labels, class_names, samples=20):\n",
    "    num_classes = len(class_names)\n",
    "    confusion_matrix = np.zeros((num_classes, num_classes))\n",
    "    for class_ind in range(num_classes):\n",
    "        pos_class_inds = np.argwhere(labels == class_ind).flatten()\n",
    "        class_samples = np.random.choice(pos_class_inds, samples)\n",
    "        for sample_ind in class_samples:\n",
    "            image_vec = vecs[sample_ind, :]\n",
    "            nn_class = calculate_nearest_class(image_vec, class_ind, vecs,\n",
    "                                               labels)\n",
    "            confusion_matrix[class_ind, nn_class] += 1\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vecs = base_model(test_images).numpy()\n",
    "confusion_matrix = calculate_confusion_matrix(test_vecs,\n",
    "                                              test_labels,\n",
    "                                              class_names,\n",
    "                                              samples=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "cmp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix,\n",
    "                             display_labels=class_names)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8));\n",
    "cmp.plot(ax=ax, xticks_rotation=\"vertical\", include_values=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diff_nearest_class(image_vec, image_class, vecs, labels):\n",
    "    '''Find the nearest different class of a single image vector'''\n",
    "    # Different class indexes\n",
    "    neg_class_inds = np.argwhere(labels != image_class).flatten()\n",
    "    # Vectors of different class\n",
    "    neg_vecs = vecs[neg_class_inds, :]\n",
    "    #\u00a0vectors are normalized: argmin_i |v -v_i| = argmax_i v_i.dot(v)\n",
    "    most_similar_ind = np.argmax(neg_vecs.dot(image_vec))\n",
    "    nearest_class = labels[neg_class_inds[most_similar_ind]]\n",
    "    return nearest_class\n",
    " \n",
    "def calculate_nearest_classes(vecs, labels, class_names, samples=20):\n",
    "    num_classes = len(class_names)\n",
    "    nearest_classes = np.zeros((num_classes, num_classes))\n",
    "    for class_ind in range(num_classes):\n",
    "        pos_class_inds = np.argwhere(labels == class_ind).flatten()\n",
    "        class_samples = np.random.choice(pos_class_inds, samples)\n",
    "        for sample_ind in class_samples:\n",
    "            image_vec = vecs[sample_ind, :]\n",
    "            nn_class = calculate_diff_nearest_class(image_vec, class_ind, vecs,\n",
    "                                                    labels)\n",
    "            nearest_classes[class_ind, nn_class] += 1\n",
    "    return nearest_classes\n",
    "\n",
    "\n",
    "test_vecs = base_model(test_images).numpy()\n",
    "nearest_classes = calculate_nearest_classes(test_vecs,\n",
    "                                              test_labels,\n",
    "                                              class_names,\n",
    "                                              samples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "cmp = ConfusionMatrixDisplay(confusion_matrix=nearest_classes, display_labels=class_names)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "cmp.plot(ax=ax, xticks_rotation=\"vertical\", include_values=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBnyx5_B0Ru6"
   },
   "source": [
    "### Question 1: Create a convolutional base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v35f_4ou0Ru7"
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "(train_images,\n",
    " train_labels), (test_images,\n",
    "                 test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# prepare train and test sets\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "\n",
    "# normalize values\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "train_images = np.expand_dims(train_images, -1)\n",
    "test_images = np.expand_dims(test_images, -1)\n",
    "\n",
    "class_names = [\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt',\n",
    "    'Sneaker', 'Bag', 'Ankle boot'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BH_3h_580Ru7",
    "outputId": "64f538ce-54ed-47a5-9174-e8b3114efbae"
   },
   "outputs": [],
   "source": [
    "train_triplets = create_triplets(train_images, train_labels, 10000)\n",
    "train_triplets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IElwTSBH0RvD",
    "outputId": "2be58a9a-67f0-4467-fec9-dc3c940c6da6"
   },
   "outputs": [],
   "source": [
    "vec_dim = 64\n",
    "\n",
    "inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "\n",
    "## convolutional layers\n",
    "# Conv Layer 1\n",
    "... = layers.Conv2D(...)(inputs)\n",
    "... = layers.MaxPooling2D(...)\n",
    "...\n",
    "\n",
    "## Flatten layer\n",
    "flat = ...\n",
    "\n",
    "\n",
    "## Output vector\n",
    "x = layers.Dense(...)\n",
    "# L2 normalize\n",
    "outputs = layers.Lambda(lambda z: tf.math.l2_normalize(z,axis=1))(x)\n",
    "\n",
    "\n",
    "base_model = tf.keras.Model(inputs=inputs, outputs=outputs, name='base_model')\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzNefw5H0Rvf"
   },
   "outputs": [],
   "source": [
    "#\u00a0Input triplets (anchor, positive, negative)  \n",
    "inputs = tf.keras.Input(shape=(3, 28, 28, 1), name='inputs')\n",
    "\n",
    "## Anchor vector\n",
    "anchor_input = inputs[:, 0, :, :, :]\n",
    "anchor_vec = base_model(anchor_input)\n",
    "\n",
    "## Positive vector\n",
    "positive_input = inputs[:, 1, :, :, :]\n",
    "positive_vec = base_model(positive_input)\n",
    "\n",
    "## Negative vector\n",
    "negative_input = inputs[:, 2, :, :, :]\n",
    "negative_vec = base_model(negative_input)\n",
    "\n",
    "## stack all the vectors dim: (3, none, vec_dim)\n",
    "outputs = tf.stack([anchor_vec, positive_vec, negative_vec])\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "THBXPAQw0Rvf",
    "outputId": "30d45b50-8a9f-405c-89c8-08e2d68b05e9"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8oN2WaJ0Rvg"
   },
   "outputs": [],
   "source": [
    "y = np.zeros(len(train_triplets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yUjlBAnV0Rvg",
    "outputId": "178e60da-92cc-4dc0-9682-65415b796558"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=triplet_loss,optimizer='adam')\n",
    "history = model.fit(train_triplets, y, epochs=8, validation_split=0.15, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqnU_AVi0Rvg"
   },
   "source": [
    "### Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "B6z3GjFL0Rvg",
    "outputId": "c338b715-b014-4764-d3c7-d2e35aa8a55b"
   },
   "outputs": [],
   "source": [
    "for class_ind in range(10):\n",
    "    print('-'*80)\n",
    "    class_name = class_names[class_ind]\n",
    "    print(class_name, class_ind)\n",
    "    class_inds = np.argwhere(test_labels == class_ind).flatten()\n",
    "    image = np.squeeze(test_images[np.random.choice(class_inds)])\n",
    "    image_vec = base_model(np.expand_dims(image, 0))\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "    print('distances:')\n",
    "    plot_images_dis(base_model, image_vec, test_images, test_labels, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vecs = base_model(test_images).numpy()\n",
    "confusion_matrix = calculate_confusion_matrix(test_vecs,\n",
    "                                              test_labels,\n",
    "                                              class_names,\n",
    "                                              samples=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "cmp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix,\n",
    "                             display_labels=class_names)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8));\n",
    "cmp.plot(ax=ax, xticks_rotation=\"vertical\", include_values=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WO29xD7x2YJA",
    "outputId": "af0e61d4-330d-4741-ed63-72c5fd33d995"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "del train_triplets, train_images, test_images\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKPZwGtC0Rvh"
   },
   "source": [
    "## Practice\n",
    "\n",
    "Repeat the process with the flower dataset, a set of **~ 3700 photographs** of flowers from **5 different classes**. Same dataset as [Introduction_to_CNN.ipynb](https://github.com/ezponda/intro_deep_learning/blob/main/class/CNN/Introduction_to_CNN.ipynb).\n",
    "\n",
    "Use a pretrained model for the base-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUDKgd2a0Rvh"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import tensorflow as tf\n",
    "dataset_url = 'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz'\n",
    "data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\n",
    "data_dir = pathlib.Path(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_7Swd6kr0Rvh",
    "outputId": "8f5ef6ac-d506-4cc9-cfd5-306ca78d3005"
   },
   "outputs": [],
   "source": [
    "image_size = (80, 80)\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,  # 80%  train, 20% validation\n",
    "    subset=\n",
    "    'training',  # 'training' o 'validation', only  with 'validation_split'\n",
    "    seed=1,\n",
    "    image_size=image_size,# Dimension (img_height, img_width) for rescaling\n",
    "    batch_size=1\n",
    ")\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=1,\n",
    "    image_size=image_size,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i, (images, labels) in enumerate(train_ds.take(9)):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[0].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[0]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eZcM4pBb8WRI",
    "outputId": "570c26ce-45a8-456b-bbdd-59d8cf7ec3c4"
   },
   "outputs": [],
   "source": [
    "train_images, train_labels = zip(*[\n",
    "    (items[0][0], items[1][0]) for items in train_ds.as_numpy_iterator()\n",
    "])\n",
    "train_images = np.array(train_images)\n",
    "train_images = train_images / 255.0\n",
    "train_labels = np.array(train_labels)\n",
    "train_images.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o8fGuAGC7hWN",
    "outputId": "0238c21d-64ce-497f-c06d-b73e7c636ee3"
   },
   "outputs": [],
   "source": [
    "test_images, test_labels = zip(*[\n",
    "    (items[0][0], items[1][0]) for items in val_ds.as_numpy_iterator()\n",
    "])\n",
    "test_images = np.array(test_images)\n",
    "test_images = test_images / 255.0\n",
    "test_labels = np.array(test_labels)\n",
    "test_images.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBpIo2_d0Rvi",
    "outputId": "b7bce07a-6bb4-4957-d780-ff12715677b7"
   },
   "outputs": [],
   "source": [
    "train_triplets = create_triplets(train_images, train_labels, 5000)\n",
    "train_triplets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\u00a0Create the base model \n",
    "Use a pre-trained model like `MobileNetV2`: [`tf.keras.applications`](https://www.tensorflow.org/api_docs/python/tf/keras/applications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vcp01iw40Rvi",
    "outputId": "22c98097-21ed-4f2c-d2ec-9f1bba174fb7"
   },
   "outputs": [],
   "source": [
    "vec_dim = 128\n",
    "\n",
    "inputs = tf.keras.Input(shape=image_size+(3,))\n",
    "\n",
    "## Pre-trained model\n",
    "pretrained_model = ...\n",
    "pretrained_model.trainable...\n",
    "### preprocess inputs\n",
    "\n",
    "## Flatten layer\n",
    "flat = ...\n",
    "\n",
    "## Output vector\n",
    "...\n",
    "# L2 normalize\n",
    "outputs = layers.Lambda(lambda z: tf.math.l2_normalize(z, axis=1))(...)\n",
    "\n",
    "base_model = tf.keras.Model(inputs=inputs, outputs=outputs, name='base_model')\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjIYXyYT0Rvj"
   },
   "outputs": [],
   "source": [
    "#\u00a0Input triplets (anchor, positive, negative)  \n",
    "inputs = tf.keras.Input(shape=(3, 80, 80, 3), name='inputs')\n",
    "\n",
    "## Anchor vector\n",
    "anchor_input = inputs[:, 0, :, :, : ]\n",
    "anchor_vec = base_model(anchor_input)\n",
    "\n",
    "## Positive vector\n",
    "positive_input = inputs[:, 1, :, :,:]\n",
    "positive_vec = base_model(positive_input)\n",
    "\n",
    "## Negative vector\n",
    "negative_input = inputs[:, 2, :, :,:]\n",
    "negative_vec = base_model(negative_input)\n",
    "\n",
    "## stack all the vectors dim: (3, none, vec_dim)\n",
    "outputs = tf.stack([anchor_vec, positive_vec, negative_vec])\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "vT428hV90Rvj",
    "outputId": "a347dda0-c4ed-4048-ba30-ebb2ac6c263e"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bcM6K8f-0Rvj",
    "outputId": "7423bb0f-bff2-4ec2-9710-ab5f1b359213"
   },
   "outputs": [],
   "source": [
    "y = np.zeros(len(train_triplets))\n",
    "model.compile(loss=triplet_loss,optimizer='adam')\n",
    "history = model.fit(train_triplets, y, epochs=1, validation_split=0.15, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\u00a0EXTRA (Optional) \n",
    "\n",
    "Every one or two epochs, recreate the training triplets by choosing the negative images closest to the anchor to accelerate convergence. \n",
    "You can use [KDTree](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "#train_vecs = base_model(train_images).numpy() \n",
    "#class_ind = 0\n",
    "#pos_class_inds = np.argwhere(train_labels == class_ind).flatten()\n",
    "#neg_class_inds = np.argwhere(train_labels != class_ind).flatten()\n",
    "def create_triplets_ordered(...\n",
    "new_triplets = create_triplets_ordered(...                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(...):\n",
    "    new_triplets = create_triplets_ordered(...\n",
    "    y = np.zeros(len(new_triplets))\n",
    "    history = model.fit(new_triplets, y, epochs=1, validation_split=0.15, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ8J9N9z0Rvk"
   },
   "source": [
    "### Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kfPPp5Lh0Rvk",
    "outputId": "bd823159-0a9b-44f3-f6a2-65fd413e3495"
   },
   "outputs": [],
   "source": [
    "for class_ind in range(len(class_names)):\n",
    "    print('-'*80)\n",
    "    class_name = class_names[class_ind]\n",
    "    print(class_name, class_ind)\n",
    "    class_inds = np.argwhere(test_labels == class_ind).flatten()\n",
    "    image = test_images[np.random.choice(class_inds)]\n",
    "    image_vec = base_model(np.expand_dims(image, 0))\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "    print('distances:')\n",
    "    plot_images_dis(base_model, image_vec, test_images, test_labels, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_xxbghpDqC-"
   },
   "outputs": [],
   "source": [
    "## find nearest image\n",
    "def find_k_nearest(image_vec, train_vecs, k=1):\n",
    "    dis_list = []\n",
    "    for i in range(len(train_vecs)):\n",
    "        dis = distance(image_vec, train_vecs[i])\n",
    "        dis_list.append((i, dis))\n",
    "    dis_list = sorted(dis_list, key=lambda z: z[1])\n",
    "    return dis_list[:k]\n",
    "\n",
    "train_vecs = base_model(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Skfs1XGJ4hBR"
   },
   "outputs": [],
   "source": [
    "ind = np.random.randint(len(test_images))\n",
    "image = np.expand_dims(test_images[ind], 0)\n",
    "image_vec = base_model(image)\n",
    "nearest = find_k_nearest(image_vec, train_vecs, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fe2kRoxbErVC",
    "outputId": "472ad88f-f655-401f-9299-3a060bcba56c"
   },
   "outputs": [],
   "source": [
    "print('Random image')\n",
    "plt.imshow(image[0])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "print()\n",
    "print('-' * 50)\n",
    "print('Nearest')\n",
    "for ind, dist in nearest:\n",
    "    img = train_images[ind]\n",
    "    label = class_names[train_labels[ind]]\n",
    "    plt.imshow(img)\n",
    "    plt.xlabel('distance:{0:1.2f}'.format(dist))\n",
    "    plt.title(label)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhHr1m073fhl"
   },
   "outputs": [],
   "source": [
    "## load new image\n",
    "# clavel\n",
    "def read_image(image_path, target_size=None, grayscale=False):\n",
    "    image = tf.keras.preprocessing.image.load_img(image_path,\n",
    "                                                  target_size=target_size,\n",
    "                                                  grayscale=grayscale)\n",
    "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    image = image.astype(np.uint8)\n",
    "    return image\n",
    "\n",
    "\n",
    "url = 'https://hips.hearstapps.com/es.h-cdn.co/mcres/images/mi-casa/terraza-jardines-porche/cuidados-del-clavel/1106673-1-esl-ES/los-cuidados-basicos-del-clavel.jpg?crop=1.00xw:0.889xh;0,0.0886xh&resize=480:*'\n",
    "image_path = tf.keras.utils.get_file(\"clavel2.jpg\", url)\n",
    "image = read_image(image_path, target_size=image_size)\n",
    "\n",
    "image = np.expand_dims(image, 0)\n",
    "image_vec = base_model(image)\n",
    "nearest = find_k_nearest(image_vec, train_vecs, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Dir_Xjqm0Rvk",
    "outputId": "e5ae4139-5d02-476a-d356-c22b9911e524"
   },
   "outputs": [],
   "source": [
    "print('Image')\n",
    "plt.imshow(image[0])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "print()\n",
    "print('-' * 50)\n",
    "print('Nearest')\n",
    "for ind, dist in nearest:\n",
    "    img = train_images[ind]\n",
    "    label = class_names[train_labels[ind]]\n",
    "    plt.imshow(img)\n",
    "    plt.xlabel('distance:{0:1.2f}'.format(dist))\n",
    "    plt.title(label)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ke4xFfNk0Rvk"
   },
   "outputs": [],
   "source": [
    "\n",
    "url = 'https://i.blogs.es/a5bd8a/amapolas-1/450_1000.jpg'\n",
    "image_path = tf.keras.utils.get_file(\"amapola.jpg\", url)\n",
    "image = read_image(image_path, target_size=image_size)\n",
    "\n",
    "image = np.expand_dims(image, 0)\n",
    "image_vec = base_model(image)\n",
    "nearest = find_k_nearest(image_vec, train_vecs, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3ZM1p3VG6P2M",
    "outputId": "6085cd6d-9ac5-4359-9481-d018eb554b7d"
   },
   "outputs": [],
   "source": [
    "print('Image')\n",
    "plt.imshow(image[0])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "print()\n",
    "print('-' * 50)\n",
    "print('Nearest')\n",
    "for ind, dist in nearest:\n",
    "    img = train_images[ind]\n",
    "    label = class_names[train_labels[ind]]\n",
    "    plt.imshow(img)\n",
    "    plt.xlabel('distance:{0:1.2f}'.format(dist))\n",
    "    plt.title(label)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Siamese_net.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}