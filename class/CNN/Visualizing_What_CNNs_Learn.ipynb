{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9151b22",
   "metadata": {},
   "source": [
    "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "<table align=\"center\">\n",
    " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/CNN/Visualizing_What_CNNs_Learn.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/CNN/Visualizing_What_CNNs_Learn.ipynb\">\n",
    "        <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52debda1",
   "metadata": {},
   "source": [
    "\n",
    "**Table of Contents**\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Accessing Layers and Weights in TensorFlow](#accessing_layers)\n",
    "3. [Visualizing Layer Outputs](#visualizing_outputs)\n",
    "4. [Methods for Interpretation](#interpretation)\n",
    "    * 4.1 [Gradient-weighted Class Activation Mapping (Grad-CAM)](#gradcam)\n",
    "    * 4.2 [Saliency Maps](#saliency_maps)\n",
    "5. [Conclusion](#conclusion)\n",
    "\n",
    "<a id='introduction'></a>\n",
    "# Introduction\n",
    "\n",
    "Convolutional Neural Networks (CNNs) have been successful in solving complex machine learning problems, particularly in image recognition tasks. Although they are highly effective, they are also often criticized as being black boxes since the learned representations are hard to interpret. Fortunately, we have several visualization techniques to shed some light on what's happening inside these networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba537dfe",
   "metadata": {},
   "source": [
    "<a id='accessing_layers'></a>\n",
    "# Accessing Layers and Weights in TensorFlow\n",
    "\n",
    "\n",
    "To visualize what CNNs learn, we first need to access the internal components of our CNN model, including its layers and weights. In TensorFlow, the layers of a model can be accessed using the .layers attribute and weights can be accessed using the .get_weights() method.\n",
    "\n",
    "Here is a simple example of how you can access these components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00e5228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load the VGG16 model\n",
    "model_VGG16 = VGG16()\n",
    "\n",
    "model_VGG16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63eb6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing layers\n",
    "for layer in model_VGG16.layers:\n",
    "    print(\"Layer Name : \", layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcee3746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing weights of a specific layer\n",
    "W, b = model_VGG16.layers[1].get_weights()\n",
    "W.shape, b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdf6ee8",
   "metadata": {},
   "source": [
    "<a id='visualizing_outputs'></a>\n",
    "## 3. Visualizing Layer Outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab57354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image_path, target_size=None, grayscale=False):\n",
    "    image = tf.keras.preprocessing.image.load_img(image_path,\n",
    "                                                  target_size=target_size,\n",
    "                                                  grayscale=grayscale)\n",
    "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    image = image.astype(np.uint8)\n",
    "    return image\n",
    "\n",
    "\n",
    "url = 'https://i.ibb.co/q5TFmqh/bird.jpg'\n",
    "image_path = tf.keras.utils.get_file(\"bird.jpg\", url)\n",
    "image = read_image(image_path, target_size=(224, 224))\n",
    "img = np.expand_dims(image, axis=0)\n",
    "plt.imshow(image, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db613d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "outputs_dict = {layer.name: layer.output for layer in model_VGG16.layers}\n",
    "\n",
    "# Set up a model that returns the activation values for every layer in\n",
    "# VGG16 (as a dict).\n",
    "feature_extractor = Model(inputs=model_VGG16.inputs, outputs=outputs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5647f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the features of the image\n",
    "features = feature_extractor(img)\n",
    "print(features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9780927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature map for first hidden layer\n",
    "feature_maps = features['block1_conv1']\n",
    "print('feature_maps first hidden layer shape: ', feature_maps.shape)\n",
    "# plot all 64 maps in an 8x8 squares\n",
    "square = 8\n",
    "ix = 1\n",
    "plt.figure(figsize=(30, 30))\n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(square, square, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in viridis or gray\n",
    "        plt.imshow(feature_maps[0, :, :, ix - 1], cmap='viridis')\n",
    "        ix += 1\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13feab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = features['block1_conv2'] \n",
    "print('feature_maps shape: ', feature_maps.shape)\n",
    "for i in range(feature_maps.shape[-1]):\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(feature_maps[0, :, :, i], cmap='viridis')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea48e483",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = features['block1_pool'] \n",
    "print('feature_maps shape: ', feature_maps.shape)\n",
    "for i in range(feature_maps.shape[-1]):\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(feature_maps[0, :, :, i], cmap='viridis')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a2dcc6",
   "metadata": {},
   "source": [
    "<a id='interpretation'></a>\n",
    "# Methods for Interpretation\n",
    "\n",
    "There are several methods for interpreting the results from CNNs. We will look into two popular ones: Grad-CAM and Saliency Maps.\n",
    "\n",
    "<a id='imagenet'></a>\n",
    "## imagenet \n",
    "\n",
    "ImageNet is a large dataset of over 14 million labeled images spanning 20,000+ categories. The images were collected from the internet and labeled by human annotators using Amazon's Mechanical Turk crowdsourcing tool.\n",
    "\n",
    "For the purpose of this tutorial, the ImageNet dataset is particularly relevant because the model we're using (VGG16) was trained on ImageNet. The dataset consists of 1000 different classes, and thus, the model is capable of recognizing 1000 different types of objects.\n",
    "\n",
    "The ImageNet class index file we download is a dictionary that maps the class indices, which are the output of the model's prediction, to human-readable labels. When we visualize the Grad-CAM heatmaps, we use these labels to understand what the model is recognizing. This is why we need the ImageNet class index in this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aad79b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# URL of the ImageNet class index\n",
    "url = 'https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json'\n",
    "\n",
    "# Send a HTTP request to the URL\n",
    "response = requests.get(url)\n",
    "print(response.status_code)\n",
    "# If the request is successful, the status code will be 200\n",
    "if response.status_code == 200:\n",
    "    # Get the content of the response\n",
    "    content = response.content\n",
    "\n",
    "    # Save the json content into a dictionary\n",
    "    CLASS_INDEX = json.loads(content)\n",
    "\n",
    "    # Save the dictionary into a local file so you don't have to download it every time\n",
    "    with open('imagenet_class_index.json', 'w') as f:\n",
    "        json.dump(CLASS_INDEX, f)\n",
    "\n",
    "    # Let's test it:\n",
    "    print(CLASS_INDEX[str(282)])  # it should print ['n02123045', 'tabby'] (a kind of cat)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to download the file. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ea5475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform CLASS_INDEX to class_name:index\n",
    "class2index = {value[1]: int(key) for key, value in CLASS_INDEX.items()}\n",
    "\n",
    "# Let's test it:\n",
    "class2index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677a370a",
   "metadata": {},
   "source": [
    "\n",
    "<a id='gradcam'></a>\n",
    "## Gradient-weighted Class Activation Mapping (Grad-CAM)\n",
    "\n",
    "Grad-CAM uses the gradients of any target concept flowing into the final convolutional layer to produce a coarse localization map of the important regions in the image. It combines the strengths of the gradient-based localization and class activation mapping to achieve\n",
    "\n",
    " better performance.\n",
    "\n",
    "The computation of Grad-CAM can be outlined as follows:\n",
    "\n",
    "1. Given an image, forward propagate it through the model to obtain the raw class scores before softmax.\n",
    "2. Compute the gradients of the class score with respect to feature maps of the final convolutional layer.\n",
    "3. Global average pool the gradients over the width and height dimensions to obtain the neuron importance weights.\n",
    "4. The weighted combination of forward activation maps is then followed by a ReLU.\n",
    "\n",
    "Mathematically, Grad-CAM can be expressed as:\n",
    "\n",
    "Let $A^k$ be the activation maps and $y^c$ be the class score. The importance weights $\\alpha^c_k$ are calculated as:\n",
    "\n",
    "$$\\alpha^c_k = \\frac{1}{Z} \\sum_i \\sum_j \\frac{\\partial y^c}{\\partial A^k_{ij}}$$\n",
    "\n",
    "where $Z$ is the total number of pixels in each feature map. The localization map $L^c_{Grad-CAM}$ is then obtained as:\n",
    "\n",
    "$$L^c_{Grad-CAM} = ReLU(\\sum_k \\alpha^c_k A^k)$$\n",
    "\n",
    "Example with tf-keras-vis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f580c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tf-keras-vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f805b562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# great_white_shark image\n",
    "url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/56/White_shark.jpg/480px-White_shark.jpg'\n",
    "image_path = tf.keras.utils.get_file(\"great_white_shark2.jpg\", url)\n",
    "image = read_image(image_path, target_size=(224, 224))\n",
    "img = np.expand_dims(image, axis=0)\n",
    "plt.imshow(image, cmap='viridis')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34273d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocess_input(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265cdb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from tf_keras_vis.gradcam_plus_plus import GradcamPlusPlus\n",
    "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
    "from tf_keras_vis.utils.scores import CategoricalScore\n",
    "\n",
    "categorical_index = class2index['great_white_shark']\n",
    "\n",
    "# Create GradCAM++ object\n",
    "gradcam = GradcamPlusPlus(model_VGG16,\n",
    "                          model_modifier=ReplaceToLinear(),\n",
    "                          clone=True)\n",
    "\n",
    "# Generate cam with GradCAM++\n",
    "cam = gradcam(CategoricalScore(categorical_index),X,\n",
    "              penultimate_layer=-1)\n",
    "\n",
    "\n",
    "plt.imshow(image)\n",
    "heatmap = np.uint8(cm.jet(cam[0])[..., :3] * 255)\n",
    "plt.imshow(heatmap, cmap='jet', alpha=0.5) # overlay\n",
    "plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
