{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "540c8652",
   "metadata": {},
   "source": [
    "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "<table align=\"center\">\n",
    " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/NLP/semantic_search_QA.ipynb\">\n",
    "        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/NLP/semantic_search_QA.ipynb\">\n",
    "        <img src=\"https://i.ibb.co/xfJbPmL/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e220629",
   "metadata": {},
   "source": [
    "# Semantic search & QA\n",
    "\n",
    "In this notebook, we'll introduce semantic search and question-answering using [`sentence-transformers`](https://www.sbert.net/), a Python library for state-of-the-art sentence, text and image embeddings. These embeddings are useful for semantic similarity tasks, such as information retrieval and question-answering systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85446b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the sentence-transformers library\n",
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef53ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import time\n",
    "import gzip\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee4a0a8",
   "metadata": {},
   "source": [
    "We'll use a pre-trained Sentence Transformer model to generate sentence embeddings. Many pre-trained models are available [here](https://www.sbert.net/docs/pretrained_models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56081ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ab530d",
   "metadata": {},
   "source": [
    "For our semantic search and question-answering task, we need a list of documents or paragraphs to search through for relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b473c53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample paragraphs\n",
    "paragraphs = [\n",
    "    \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\",\n",
    "    \"The Statue of Liberty is a colossal neoclassical sculpture on Liberty Island in New York Harbor within New York City, in the United States.\",\n",
    "    \"The Great Wall of China is a series of fortifications made of stone, brick, tamped earth, wood, and other materials, generally built along an east-to-west line across the historical northern borders of China.\",\n",
    "    \"The Colosseum, also known as the Flavian Amphitheatre, is an oval amphitheatre in the centre of the city of Rome, Italy.\",\n",
    "    \"The Taj Mahal is an ivory-white marble mausoleum on the southern bank of the river Yamuna in the Indian city of Agra.\"\n",
    "]\n",
    "\n",
    "paragraphs = np.array(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad2ca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for paragraphs\n",
    "corpus_embeddings = model.encode(paragraphs)\n",
    "print(corpus_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a5756f",
   "metadata": {},
   "source": [
    "Now, let's define a function to perform semantic search, given a query and a list of paragraph embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a9592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, model, corpus_embeddings, paragraphs, top_k=2):\n",
    "    query_embedding = model.encode([query])[0]\n",
    "    similarities = cosine_similarity([query_embedding], corpus_embeddings)[0]\n",
    "    indexes = np.argpartition(similarities, -top_k)[-top_k:]\n",
    "    indexes = indexes[np.argsort(-similarities[indexes])]\n",
    "    print(f\"Input query: {query}\")\n",
    "    print()\n",
    "    for text, sim in zip(list(paragraphs[indexes]), similarities[indexes].tolist()):\n",
    "        print(f\"{sim:.3f}\\t{text}\")\n",
    "              \n",
    "\n",
    "semantic_search('Where is the Colosseum', model, corpus_embeddings, paragraphs, top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df5645",
   "metadata": {},
   "source": [
    "## Multilingual models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b0cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try in other languages\n",
    "semantic_search('¿Dónde está el Coliseo?', model, corpus_embeddings, paragraphs, top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d55f723",
   "metadata": {},
   "source": [
    "We have multilinguals models available [here](https://www.sbert.net/docs/pretrained_models.html#multi-lingual-models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea38223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use multilingual models \n",
    "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "multi_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7e03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_corpus_embeddings = multi_model.encode(paragraphs)\n",
    "print(multi_corpus_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d89f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_search('¿Dónde está el Coliseo?', multi_model, multi_corpus_embeddings, paragraphs, top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08858a4b",
   "metadata": {},
   "source": [
    "## Wikipedia semantic search\n",
    "\n",
    "As dataset, we use Simple English Wikipedia. Compared to the full English wikipedia, it has only\n",
    "about 170k articles. We split these articles into paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aef3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_filepath = 'data/simplewiki-2020-11-01.jsonl.gz'\n",
    "\n",
    "if not os.path.exists(wikipedia_filepath):\n",
    "    util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath)\n",
    "\n",
    "passages = []\n",
    "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        data = json.loads(line.strip())\n",
    "        for paragraph in data['paragraphs']:\n",
    "            # We encode the passages as [title, text]\n",
    "            passages.append(data['title']+':  '+ paragraph)\n",
    "\n",
    "# If you like, you can also limit the number of passages you want to use\n",
    "print(\"Passages:\", len(passages))\n",
    "print(passages[0])\n",
    "print(passages[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa111507",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_passages = np.array(passages[:5000])\n",
    "reduced_passages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4cc331",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embeddings = model.encode(reduced_passages, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d1260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_search('Best american actor', model, corpus_embeddings, reduced_passages, top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad4f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_search('Number countries Europe', model, corpus_embeddings, reduced_passages, top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327dce75",
   "metadata": {},
   "source": [
    "### Question1: Load a different pre-trained Sentence Transformer model and compare its performance to the last model on the same set of paragraphs and queries. Which model performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043b5d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a different pre-trained model, generate embeddings, and test with the same queries\n",
    "model_name = ...\n",
    "new_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb3ca16",
   "metadata": {},
   "source": [
    "## Question 2: Find text duplicates\n",
    "\n",
    "Try to find duplicate or near-duplicate texts in a given corpus based on their semantic similarity using sentence-transformers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e63124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The quick brown fox leaps over the lazy dog.\",\n",
    "    \"The sky is blue, and the grass is green.\",\n",
    "    \"The grass is green, and the sky is blue.\",\n",
    "    \"It's a sunny day today.\",\n",
    "    \"The weather is sunny today.\",\n",
    "    \"She was wearing a beautiful red dress.\",\n",
    "    \"She had on a gorgeous red dress.\",\n",
    "    \"I'm going to the supermarket to buy some groceries.\",\n",
    "    \"I'm heading to the supermarket to purchase some groceries.\",\n",
    "    \"He didn't like the movie because it was too long.\",\n",
    "    \"He disliked the movie as it was too lengthy.\",\n",
    "    \"The train was delayed due to technical issues.\",\n",
    "    \"Technical issues caused the train to be delayed.\",\n",
    "    \"I'll have a cup of coffee with milk and sugar, please.\",\n",
    "    \"Can I get a coffee with milk and sugar, please?\",\n",
    "    \"The conference was very informative and interesting.\",\n",
    "    \"The conference turned out to be interesting and informative.\",\n",
    "    \"He enjoys listening to classical music in his free time.\",\n",
    "    \"In his leisure time, he likes to listen to classical music.\",\n",
    "    \"Please make sure you turn off the lights before leaving.\",\n",
    "    \"Before leaving, ensure that you switch off the lights.\"\n",
    "]\n",
    "\n",
    "corpus += [\n",
    "    \"The boy was delighted with the gift he received.\",\n",
    "    \"Receiving the present made the young lad ecstatic.\",\n",
    "    \"She has a preference for Italian cuisine.\",\n",
    "    \"Her favorite type of food is from Italy.\",\n",
    "    \"The software engineer resolved the issue by modifying the code.\",\n",
    "    \"By altering the programming, the tech expert fixed the problem.\",\n",
    "    \"Due to the inclement weather, the baseball game was postponed.\",\n",
    "    \"The baseball match was rescheduled because of bad weather conditions.\",\n",
    "    \"The house was engulfed in a raging fire.\",\n",
    "    \"Flames rapidly consumed the residence.\",\n",
    "    \"He is constantly browsing the internet for the latest news.\",\n",
    "    \"He frequently scours the web to stay updated on current events.\",\n",
    "    \"The puppy was playing with a toy in the garden.\",\n",
    "    \"In the yard, the young dog was frolicking with its plaything.\",\n",
    "    \"The artist painted a beautiful landscape on the canvas.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b500687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize the SentenceTransformer model\n",
    "model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe93930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Obtain corpus embeddings\n",
    "embeddings = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce3fd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Calculate similarity and find duplicates\n",
    "\n",
    "# TODO: Define a similarity threshold\n",
    "similarity_threshold = ...\n",
    "\n",
    "# TODO: Iterate over each pair of embeddings in the corpus\n",
    "# Calculate the cosine similarity between the embeddings\n",
    "# If the similarity is above the threshold, add the sentences to the duplicates list\n",
    "duplicates = []\n",
    "\n",
    "for i, emb1 in enumerate(embeddings):\n",
    "    for j, emb2 in enumerate(embeddings[i + 1:]):\n",
    "        similarity = cosine_similarity([emb1], [emb2])[0][0]\n",
    "        if ...:\n",
    "            duplicates.append((corpus[i], corpus[i + j + 1], similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b17148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicate sentences:\")\n",
    "for sent1, sent2, sim in duplicates:\n",
    "    print(f\"{sent1} | {sent2} | Similarity: {sim:.2f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
