{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a687c92e",
   "metadata": {},
   "source": [
    "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "<table align=\"center\">\n",
    " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/NLP/Image_search.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/NLP/Image_search.ipynb\">\n",
    "        <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72febd9",
   "metadata": {},
   "source": [
    "# Introduction to Image Similarity\n",
    "\n",
    "In this notebook, we'll introduce image search using [`sentence-transformers`](https://www.sbert.net/), a Python library for state-of-the-art sentence, text and image embeddings.\n",
    "\n",
    "## What is Image Similarity?\n",
    "\n",
    "Image similarity refers to the process of finding images that are visually alike. This can range from finding near-identical duplicates to grouping images based on thematic resemblance. The implications of this technology are profound, as it underpins systems in:\n",
    "\n",
    "- **Visual Search:** Retailers and online marketplaces use image similarity to provide product recommendations based on user-uploaded photos. This technology enhances the shopping experience by allowing users to search for products using images instead of words.\n",
    "\n",
    "- **Content Discovery:** Social media platforms and content management systems rely on image similarity to categorize and recommend content, helping users discover new posts related to what they already like.\n",
    "\n",
    "- **Digital Archiving:** In libraries and archives, image similarity helps in organizing, indexing, and retrieving visual content from vast databases, making it easier to find historical documents and artworks.\n",
    "\n",
    "- **Security and Surveillance:** Image similarity algorithms can identify objects or persons of interest across different video frames or locations, contributing to safety and law enforcement efforts.\n",
    "\n",
    "- **Healthcare:** In medical imaging, similarity measures can help in identifying similar case histories, understanding disease progression, and even assisting in diagnosis by comparing patient scans with a database of known conditions.\n",
    "\n",
    "## Why is it Challenging?\n",
    "\n",
    "Images can vary in size, angle, lighting, and even be partially obscured. Traditional methods that rely on exact matches fall short in providing relevant results under these conditions. Hence, modern image similarity techniques employ deep learning, particularly leveraging models like CLIP (Contrastive Language-Image Pretraining) developed by OpenAI, to understand and quantify the likeness in a way that mimics human perception.\n",
    "\n",
    "## Objectives of this Tutorial\n",
    "\n",
    "In this tutorial, we will delve into how deep learning, especially through the use of Sentence Transformers and the CLIP model, enables us to map images and texts into a shared vector space. This mapping allows us to perform nuanced search and retrieval tasks, offering a bridge between textual descriptions and visual content.\n",
    "\n",
    "By the end of this session, you'll understand how to:\n",
    "- Utilize the CLIP model to create embeddings for images and text.\n",
    "- Implement an image search system that can find similar images based on text queries.\n",
    "- Explore practical applications and considerations in deploying image similarity models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c43ec",
   "metadata": {},
   "source": [
    "# Image search\n",
    "\n",
    "In this notebook, we'll introduce image search using Sentence Transformers, by mapping images and texts into the same vector space. This enables us to perform search and retrieval tasks for images based on textual descriptions.\n",
    "\n",
    "To achieve this, we'll utilize the [CLIP (Contrastive Language-Image Pretraining)](https://openai.com/research/clip) model, which is designed to learn a joint embedding space for both images and texts.\n",
    "\n",
    "Contrastive Language-Image Pretraining (CLIP) is an AI model developed by OpenAI. It is designed to learn from a wide range of tasks by leveraging the connection between natural language and images.\n",
    "\n",
    "1. Multimodal Learning: CLIP is a multimodal model that can understand both images and text. It is pretrained on a large dataset containing pairs of images and their associated text captions, learning to associate visual concepts with natural language.\n",
    "\n",
    "2. Contrastive Learning: CLIP learns by optimizing a contrastive objective. It is trained to recognize which image-caption pairs are correct among a set of negative examples. By learning to score the correct image-text pairs higher than incorrect ones, the model learns a useful representation for both modalities.\n",
    "\n",
    "3. Architecture: CLIP uses a Transformer-based architecture for processing text and a Vision Transformer or ResNet architecture for processing images. The image and text encoders are jointly trained, allowing the model to align both modalities in a shared embedding space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e44036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the sentence-transformers library\n",
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985e3a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "import pickle\n",
    "import zipfile\n",
    "import copy\n",
    "from IPython.display import display\n",
    "from IPython.display import Image as IPImage\n",
    "import os\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa59f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we load the respective CLIP model\n",
    "model_name = 'clip-ViT-B-32'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b22ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import StringIO, BytesIO\n",
    "\n",
    "def get_image_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300f0287",
   "metadata": {},
   "source": [
    "For searching images, we need an image set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73382b00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_url_path = 'https://github.com/ezponda/intro_deep_learning/raw/main/images/'\n",
    "img_urls = [\n",
    "    f'{img_url_path}eiffel_tower.jpeg',\n",
    "    f'{img_url_path}taj_mahal.jpeg',\n",
    "    f'{img_url_path}colosseum.jpeg',\n",
    "    f'{img_url_path}great_wall_of_china.jpeg',\n",
    "    f'{img_url_path}statue_of_liberty.jpeg',\n",
    "]\n",
    "images = [get_image_from_url(url) for url in img_urls]\n",
    "\n",
    "print('Sample images: ')\n",
    "for url, image in zip(img_urls, images):\n",
    "    print('_'*50)\n",
    "    print(f'url: {url}')\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embeddings = model.encode(images,\n",
    "                       batch_size=128,\n",
    "                       convert_to_tensor=True,\n",
    "                       show_progress_bar=True)\n",
    "img_embeddings = img_embeddings.cpu()\n",
    "print(img_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb617fb8",
   "metadata": {},
   "source": [
    "Now, let's define a function to perform image search, given a query and a list of image embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e17ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "def image_search(query: str, model: SentenceTransformer, img_embeddings: np.ndarray, \n",
    "                 images: List[Image.Image], top_k: int = 2) -> None:\n",
    "    \"\"\"Perform an image search given a text query.\n",
    "\n",
    "    This function computes the cosine similarity between the query embedding and the image embeddings,\n",
    "    retrieves the top_k images with the highest similarity, and displays them.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query as a text string.\n",
    "        model (SentenceTransformer): The SentenceTransformer model used to encode the text and images.\n",
    "        img_embeddings (np.ndarray): Precomputed embeddings for the images.\n",
    "        images (List[Image.Image]): A list of PIL Image objects.\n",
    "        top_k (int): The number of top results to display.\n",
    "\n",
    "    \"\"\"\n",
    "    # Encode the query to get the embeddings\n",
    "    query_embedding = model.encode([query])[0]\n",
    "    \n",
    "    # Compute the cosine similarities between the query embedding and the image embeddings\n",
    "    similarities = cosine_similarity([query_embedding], img_embeddings)[0]\n",
    "    \n",
    "    # Get the indices of the top_k most similar images\n",
    "    top_k_indices = np.argsort(-similarities)[:top_k]\n",
    "    \n",
    "    # Print the input query for reference\n",
    "    print(f\"Input query: {query}\\n\")\n",
    "    \n",
    "    # Display the top_k similar images along with their similarity scores\n",
    "    for index in top_k_indices:\n",
    "        print('_' * 50)\n",
    "        print(f\"Similarity Score: {similarities[index]:.4f}\")  # Improved readability with formatting\n",
    "        display(images[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf9c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_search('A building in Paris', model, img_embeddings, images, top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7da09ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_search('Find me an image of a famous monument in India', model, img_embeddings, images, top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a8391",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_search('A building in China', model, img_embeddings, images, top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdb5270",
   "metadata": {},
   "source": [
    "## Unsplash subset dataset\n",
    "\n",
    "[Unsplash](https://unsplash.com/data) is a collaborative image dataset openly shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6420491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we get about 25k images from Unsplash \n",
    "img_folder = './photos/'\n",
    "if not os.path.exists(img_folder) or len(os.listdir(img_folder)) == 0:\n",
    "    os.makedirs(img_folder, exist_ok=True)\n",
    "    \n",
    "    photo_filename = 'unsplash-25k-photos.zip'\n",
    "    if not os.path.exists(photo_filename):   #Download dataset if does not exist\n",
    "        util.http_get('http://sbert.net/datasets/'+photo_filename, photo_filename)\n",
    "        \n",
    "    #Extract all images\n",
    "    with zipfile.ZipFile(photo_filename, 'r') as zf:\n",
    "        for member in tqdm(zf.infolist(), desc='Extracting'):\n",
    "            zf.extract(member, img_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58298e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we need to compute the embeddings\n",
    "# To speed things up, we destribute pre-computed embeddings\n",
    "# Otherwise you can also encode the images yourself.\n",
    "# To encode an image, you can use the following code:\n",
    "# from PIL import Image\n",
    "# img_emb = model.encode(Image.open(filepath))\n",
    "def read_image_from_path(file_path):\n",
    "    img = Image.open(file_path)\n",
    "    return img\n",
    "\n",
    "use_precomputed_embeddings = True\n",
    "\n",
    "if use_precomputed_embeddings: \n",
    "    emb_filename = 'unsplash-25k-photos-embeddings.pkl'\n",
    "    if not os.path.exists(emb_filename):   #Download dataset if does not exist\n",
    "        util.http_get('http://sbert.net/datasets/'+emb_filename, emb_filename)\n",
    "        \n",
    "    with open(emb_filename, 'rb') as fIn:\n",
    "        img_names, img_embeddings = pickle.load(fIn)  \n",
    "    \n",
    "\n",
    "    print(\"Images:\", len(img_names))\n",
    "else:\n",
    "    img_names = list(glob.glob('photos/*.jpg'))[:5_000]\n",
    "    print(\"Images:\", len(img_names))\n",
    "    images = [read_image_from_path(img_name) for img_name in  img_names]\n",
    "    img_embeddings = model.encode(images, batch_size=128, convert_to_tensor=True, show_progress_bar=True)\n",
    "    img_embeddings = img_embeddings.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce3a523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import copy\n",
    "\n",
    "def image_search_from_path(query, model: SentenceTransformer, img_embeddings: np.ndarray, \n",
    "                           img_folder: str, img_names: List[str], top_k: int = 2) -> None:\n",
    "    \"\"\"Perform an image search for a given textual query within a set of images located in a specified folder.\n",
    "\n",
    "    Args:\n",
    "        query (str or Image.Image): The textual query for searching similar images.\n",
    "        model (SentenceTransformer): The SentenceTransformer model used for encoding.\n",
    "        img_embeddings (np.ndarray): The precomputed embeddings of the images.\n",
    "        img_folder (str): The folder where images are stored.\n",
    "        img_names (List[str]): The filenames of the images.\n",
    "        top_k (int): The number of top results to return.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generate the embedding for the query\n",
    "        query_embedding = model.encode([query])[0]\n",
    "        # Calculate the similarities between the query embedding and image embeddings\n",
    "        similarities = cosine_similarity([query_embedding], img_embeddings)[0]\n",
    "        # Identify the indexes of the top_k most similar images\n",
    "        indexes = np.argpartition(similarities, -top_k)[-top_k:]\n",
    "        indexes = indexes[np.argsort(-similarities[indexes])]\n",
    "\n",
    "        print(f\"Input query: {query}\\n\")\n",
    "        for index in indexes:\n",
    "            similarity_score = similarities[index]\n",
    "            image_name = img_names[index]\n",
    "            image_path = os.path.join(img_folder, image_name)\n",
    "            try:\n",
    "                with Image.open(image_path) as img:\n",
    "                    print('_' * 50)\n",
    "                    print(f\"Similarity: {similarity_score:.4f}\")\n",
    "                    display(copy.deepcopy(img))\n",
    "            except Exception as e:\n",
    "                print(f\"Error displaying image {image_name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in image search: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4efc717",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_search_from_path('A building in Paris', model, img_embeddings, img_folder, img_names, top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833f2fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_search_from_path('A building in China', model, img_embeddings, img_folder, img_names, top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4c1443",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_search_from_path('A building in China', model, img_embeddings, img_folder, img_names, top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35453318",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_search_from_path('Two dogs playing in the snow', model, img_embeddings, img_folder, img_names, top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fee3925",
   "metadata": {},
   "source": [
    "## Image-to-Image Search\n",
    "You can use the method also for image-to-image search.\n",
    "\n",
    "To achieve this, you pass `get_image_from_url(url)` to the search method.\n",
    "\n",
    "It will then return similar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = get_image_from_url(img_urls[0])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886bf68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_search_from_path(img, model, img_embeddings, img_folder, img_names, top_k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
