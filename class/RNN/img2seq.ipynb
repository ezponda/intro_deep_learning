{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca79de4",
   "metadata": {},
   "source": [
    "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "<table align=\"center\">\n",
    " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/RNN/img2seq.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/RNN/img2seq.ipynb\">\n",
    "        <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f08e7",
   "metadata": {
    "id": "5OiQmfbX29mA"
   },
   "source": [
    "##\u00a0Generate CAPTCHA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac5f67",
   "metadata": {
    "id": "willing-lexington"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebd7212",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "virgin-potential",
    "outputId": "8d1d35dc-4b81-4272-af35-089795928694"
   },
   "outputs": [],
   "source": [
    "chars = set()\n",
    "#chars.update(string.ascii_lowercase)\n",
    "#chars.update(string.ascii_uppercase)\n",
    "chars.update({str(i) for i in range(10)})\n",
    "chars = sorted(chars)\n",
    "print('Number of chars: {0}, chars: {1}'.format(len(chars), chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec0c702",
   "metadata": {
    "id": "intelligent-employment"
   },
   "outputs": [],
   "source": [
    "img_dir = './captcha/'\n",
    "if not os.path.exists(img_dir):\n",
    "    os.makedirs(img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa08662e",
   "metadata": {
    "id": "cEAoZXmdn8nk"
   },
   "outputs": [],
   "source": [
    "#!pip install captcha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c3974c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "limited-bottle",
    "outputId": "f4abd8d8-8c96-4d85-af8f-acd214b17e3b"
   },
   "outputs": [],
   "source": [
    "from captcha.image import ImageCaptcha\n",
    "import uuid\n",
    "create_dataset = True\n",
    "captcha_len = 3\n",
    "width = 40 + 20 * captcha_len\n",
    "height = 100\n",
    "n_images = 40000\n",
    "if create_dataset:\n",
    "    image = ImageCaptcha(width = width, height = height)\n",
    "    print('Sample captcha str', np.random.choice(chars, captcha_len))\n",
    "    seen = set()\n",
    "    for _ in tqdm(range(n_images)):\n",
    "        combi = np.random.choice(chars, captcha_len)\n",
    "        captcha = ''.join(combi)\n",
    "        image.write(captcha, '{0}{1}_{2}.png'.format(img_dir, captcha, uuid.uuid4()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e29c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "manufactured-worcester",
    "outputId": "3ea40062-b23b-4903-cb2c-e14d96c5ebe8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Number of captchas', len(os.listdir(img_dir)))\n",
    "print('Some captchas', os.listdir(img_dir)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba99bfc8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "future-idaho",
    "outputId": "45ea9621-4e37-4d0c-ceae-607a1e4458d1"
   },
   "outputs": [],
   "source": [
    "## Plot first sample\n",
    "name = os.listdir(img_dir)[0]\n",
    "x = tf.keras.preprocessing.image.load_img(os.path.join(img_dir, name))\n",
    "x = tf.keras.preprocessing.image.img_to_array(x).astype(np.uint8)\n",
    "print('image shape: ', x.shape)\n",
    "plt.imshow(x)\n",
    "plt.title(name.split('_')[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f208a3",
   "metadata": {
    "id": "BzoKTysv3P37"
   },
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576785a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "vertical-homework",
    "outputId": "ca8777c6-ed29-474c-e3fa-22ec22a478cf"
   },
   "outputs": [],
   "source": [
    "data_dir = img_dir\n",
    "# Get list of all the images\n",
    "images, labels = zip(*[(os.path.join(img_dir, name), name.split('_')[0])\n",
    "                       for name in os.listdir(img_dir)])\n",
    "images, labels = (np.array(list(images)), np.array(list(labels)))\n",
    "characters = sorted(set(char for label in labels for char in label))\n",
    "\n",
    "print(\"Number of images found: \", len(images))\n",
    "print(\"Number of labels found: \", len(labels))\n",
    "print(\"Number of unique characters: \", len(characters))\n",
    "print(\"Characters present: \", characters)\n",
    "display(Image(images[0]))\n",
    "print('captcha:', labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd04f75f",
   "metadata": {
    "id": "rVZKjkVOpRbN"
   },
   "source": [
    "### Characters processing\n",
    "For converting the characters to one-hot encoding, we will use [tf.keras.layers.experimental.preprocessing.StringLookup\n",
    "](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/StringLookup). \n",
    "```python\n",
    "tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    max_tokens=None, num_oov_indices=1, mask_token='',\n",
    "    oov_token='[UNK]', vocabulary=None, encoding=None, invert=False,\n",
    "    **kwargs\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559cc233",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ultimate-calculator",
    "outputId": "5eb049d0-3a4d-40e2-a7eb-e309d3c574b5"
   },
   "outputs": [],
   "source": [
    "# Mapping characters to integers\n",
    "char_to_num = layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=list(characters), num_oov_indices=0, mask_token=None\n",
    ")\n",
    "\n",
    "# Mapping integers back to original characters\n",
    "num_to_char = layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), num_oov_indices=0,\n",
    "    mask_token=None, invert=True\n",
    ")\n",
    "\n",
    "\n",
    "train_samples = int(0.7 * len(images))\n",
    "val_split = int(0.8 * len(images))\n",
    "x_train, y_train = images[:train_samples], labels[:train_samples]\n",
    "x_val, y_val = images[train_samples:val_split], labels[train_samples:val_split]\n",
    "x_test, y_test = images[val_split:], labels[val_split:]\n",
    "print('x_train, y_train shape: ', x_train.shape, y_train.shape)\n",
    "\n",
    "def encode_single_sample(img_path, label):\n",
    "    \n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.io.decode_png(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n",
    "    # 7. Return a dict as our model is expecting two inputs\n",
    "    zeros = tf.zeros((captcha_len,1))\n",
    "    return ((img, zeros), label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e0f443",
   "metadata": {
    "id": "valuable-infection"
   },
   "source": [
    "###\u00a0Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575a5ac",
   "metadata": {
    "id": "compatible-thing"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).map(encode_single_sample)\n",
    "train_dataset = train_dataset.batch(batch_size).cache().shuffle(50).prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).map(encode_single_sample)\n",
    "validation_dataset = validation_dataset.batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).map(encode_single_sample)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ae321",
   "metadata": {
    "id": "homeless-bibliography"
   },
   "outputs": [],
   "source": [
    "# plot samples\n",
    "_, ax = plt.subplots(4, 4, figsize=(10, 10))\n",
    "for (images_batch, zeros_batch), labels_batch in train_dataset.take(1):\n",
    "    for i in range(16):\n",
    "        img = (images_batch[i] * 255).numpy().astype(\"uint8\")\n",
    "        label = tf.strings.reduce_join(num_to_char(labels_batch[i])).numpy().decode(\"utf-8\")\n",
    "        ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n",
    "        ax[i // 4, i % 4].set_title(label)\n",
    "        ax[i // 4, i % 4].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f97ea0",
   "metadata": {},
   "source": [
    "## Questions 1: Create a model like a seq2seq\n",
    "Study the impact of `encoder_vec_dim` on the performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50542b53",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "comparable-trade",
    "outputId": "96049b86-b3e5-4d7c-fcbb-9879038773a9"
   },
   "outputs": [],
   "source": [
    "encoder_vec_dim = ...  # dimension of the encoder vector\n",
    "\n",
    "# Encoder\n",
    "encoder_input = tf.keras.Input(shape=(height, width, 3), name='encoder_input')\n",
    "\n",
    "## Convolution + pooling layers\n",
    "...\n",
    "## Flatten()\n",
    "...\n",
    "\n",
    "\n",
    "\n",
    "# encoded_vector\n",
    "x = layers.Dense(encoder_vec_dim, activation='relu')(...)\n",
    "encoded_vector = [x, x]\n",
    "\n",
    "\n",
    "\n",
    "# Decoder: encoded_vector is the input state to the first decoder RNN\n",
    "decoder_input = tf.keras.Input(shape=(captcha_len, 1), name='decoder_input')\n",
    "decoder_output = layers.LSTM(encoder_vec_dim,\n",
    "                             return_sequences=True,\n",
    "                             name=\"decoder\")(decoder_input,\n",
    "                                             initial_state=encoded_vector)\n",
    "output = layers.TimeDistributed(\n",
    "    layers.Dense(len(characters) + 1, activation='softmax'))(decoder_output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = tf.keras.Model([encoder_input, decoder_input], output)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ffa7ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "magnetic-nothing",
    "outputId": "e1dfc601-d8b2-4893-8b86-c34c9f9598c1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d904d16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dense-greek",
    "outputId": "92d3385b-0a3e-4541-f336-3591bb9ed45a"
   },
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "early_stopping_patience = 50\n",
    "# Add early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c6c755",
   "metadata": {},
   "source": [
    "###\u00a0Evaluate Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb99735",
   "metadata": {
    "id": "VrWcPKBPwRw_"
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(test_dataset, verbose=1)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e3d3b",
   "metadata": {
    "id": "psychological-blond"
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(4, 4, figsize=(12, 12))\n",
    "for (images_batch, zeros_batch), labels_batch in test_dataset.take(1):\n",
    "    y_preds = model.predict([images_batch, zeros_batch]).argmax(-1)\n",
    "    for i in range(16):\n",
    "        img = (images_batch[i] * 255).numpy().astype(\"uint8\")\n",
    "        label = tf.strings.reduce_join(num_to_char(\n",
    "            labels_batch[i])).numpy().decode(\"utf-8\")\n",
    "        label_pred = tf.strings.reduce_join(num_to_char(\n",
    "            y_preds[i])).numpy().decode(\"utf-8\")\n",
    "        ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n",
    "        if label == label_pred:\n",
    "            ax[i // 4,\n",
    "               i % 4].set_title('real:{0}, pred:{1}'.format(label, label_pred),\n",
    "                                color='green')\n",
    "        else:\n",
    "            ax[i // 4,\n",
    "               i % 4].set_title('real:{0}, pred:{1}'.format(label, label_pred),\n",
    "                                color='red')\n",
    "        ax[i // 4, i % 4].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eaf1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained_model = tf.keras.applications.MobileNetV2(input_shape=(width, height, 3), include_top=False)\n",
    "#pretrained_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a68075",
   "metadata": {},
   "source": [
    "### Extra: Visual Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65b777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # attention_hidden_layer shape == (batch_size, 64, units)\n",
    "        attention_hidden_layer = (\n",
    "            tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # score shape == (batch_size, 64, 1)\n",
    "        # This gives you an unnormalized score for each image feature.\n",
    "        score = self.V(attention_hidden_layer)\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))\n",
    "\n",
    "embedding_dim = 16\n",
    "vocab_size = len(characters) + 1\n",
    "units = 32\n",
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                            reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab55fd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] *\n",
    "                               target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc91f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(1, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, ((images_batch, _), labels_batch)) in enumerate(train_dataset):\n",
    "        batch_loss, t_loss = train_step(images_batch, labels_batch)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                epoch + 1, batch,\n",
    "                batch_loss.numpy() / int(labels_batch.shape[1])))\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print('Epoch {} Loss {:.6f}'.format(epoch + 1, total_loss / num_steps))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76ce2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f18200",
   "metadata": {},
   "outputs": [],
   "source": [
    " in train_dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c3d132",
   "metadata": {},
   "source": [
    "##\u00a0Extra: Audio Captcha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61531138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "aud_dir = './captcha_audio/'\n",
    "if not os.path.exists(aud_dir):\n",
    "    os.makedirs(aud_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be316b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captcha.image import ImageCaptcha\n",
    "from captcha.audio import AudioCaptcha\n",
    "\n",
    "audio = AudioCaptcha()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba4c6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wavio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b23e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import wavio\n",
    "captcha_len = 3\n",
    "n_audios = 50\n",
    "audio = AudioCaptcha()\n",
    "print('Sample captcha str', np.random.choice(chars, captcha_len))\n",
    "seen = set()\n",
    "for _ in tqdm(range(n_audios)):\n",
    "    combi = np.random.choice(chars, captcha_len)\n",
    "    captcha = ''.join(combi)\n",
    "    captcha_path = '{0}{1}_{2}.wav'.format(aud_dir, captcha, uuid.uuid4())\n",
    "    audio.write(captcha, captcha_path)\n",
    "    \n",
    "    wav = wavio.read(captcha_path).data\n",
    "    # convert to 16-bits\n",
    "    max_val = 2 ** 16 - 1 \n",
    "    wav = (wav / 255.0 * max_val).astype(np.int16)\n",
    "    wav_tf = tf.audio.encode_wav(\n",
    "    wav, 8000, name=None\n",
    "    )\n",
    "    os. remove(captcha_path)\n",
    "    tf.io.write_file(\n",
    "    captcha_path, wav_tf, name=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of captchas', len(os.listdir(aud_dir)))\n",
    "print('Some captchas', os.listdir(aud_dir)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca50d84",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "manufactured-worcester",
    "outputId": "3ea40062-b23b-4903-cb2c-e14d96c5ebe8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_path = os.path.join(aud_dir, os.listdir(aud_dir)[0]) \n",
    "audio_binary = tf.io.read_file(file_path)\n",
    "audio, _ = tf.audio.decode_wav(audio_binary)\n",
    "print(audio.shape)\n",
    "waveform = tf.squeeze(audio, axis=-1)\n",
    "print(waveform.shape)\n",
    "print('captcha',os.listdir(aud_dir)[0].split('_')[0])\n",
    "display.display(display.Audio(waveform, rate=8000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27fd03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all the images\n",
    "audios, labels = zip(*[(os.path.join(aud_dir, name), name.split('_')[0])\n",
    "                       for name in os.listdir(aud_dir)])\n",
    "audios, labels = (np.array(list(audios)), np.array(list(labels)))\n",
    "characters = sorted(set(char for label in labels for char in label))\n",
    "print(\"Number of audios found: \", len(audios))\n",
    "print(\"Number of labels found: \", len(labels))\n",
    "print(\"Number of unique characters: \", len(characters))\n",
    "print(\"Characters present: \", characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c12f67c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ultimate-calculator",
    "outputId": "5eb049d0-3a4d-40e2-a7eb-e309d3c574b5"
   },
   "outputs": [],
   "source": [
    "# Mapping characters to integers\n",
    "char_to_num = layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=list(characters), num_oov_indices=0, mask_token=None\n",
    ")\n",
    "\n",
    "# Mapping integers back to original characters\n",
    "num_to_char = layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_samples = int(0.7 * len(audios))\n",
    "val_split = int(0.8 * len(audios))\n",
    "x_train, y_train = audios[:train_samples], labels[:train_samples]\n",
    "x_val, y_val = audios[train_samples:val_split], labels[train_samples:val_split]\n",
    "x_test, y_test = audios[val_split:], labels[val_split:]\n",
    "print('x_train, y_train shape: ', x_train.shape, y_train.shape)\n",
    "\n",
    "def encode_single_sample(aud_path, label):\n",
    "    \n",
    "    audio_binary = tf.io.read_file(aud_path)\n",
    "    audio, _ = tf.audio.decode_wav(audio_binary)\n",
    "    waveform = tf.squeeze(audio, axis=-1)[:70000]\n",
    "    \n",
    "    zero_padding = tf.zeros([70000] - tf.shape(waveform), dtype=tf.float32)\n",
    "\n",
    "    # Concatenate audio with padding so that all audio clips will be of the\n",
    "    # same length\n",
    "    waveform = tf.cast(waveform, tf.float32)\n",
    "    equal_length = tf.concat([waveform, zero_padding], 0)\n",
    "    \n",
    "    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n",
    "    # 7. Return a dict as our model is expecting two inputs\n",
    "    zeros = tf.zeros((captcha_len,1))\n",
    "    return ((equal_length, zeros), label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849461aa",
   "metadata": {
    "id": "valuable-infection"
   },
   "source": [
    "###\u00a0Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31d31e5",
   "metadata": {
    "id": "compatible-thing"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).map(encode_single_sample)\n",
    "train_dataset = train_dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE).batch(batch_size)\n",
    "    \n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).map(encode_single_sample)\n",
    "validation_dataset = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE).batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).map(encode_single_sample)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836812aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (audios_batch, zeros_batch), labels_batch in train_dataset.take(1):\n",
    "    for i in range(4):\n",
    "        label = tf.strings.reduce_join(num_to_char(labels_batch[i])).numpy().decode(\"utf-8\")\n",
    "        print('captcha',label)\n",
    "        display.display(display.Audio(audios_batch[i], rate=8000))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "img2seq.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}