{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "<table align=\"center\">\n",
    " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/RNN/Introduction_to_RNN_Time_Series.ipynb\">\n",
    "        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/RNN/Introduction_to_RNN_Time_Series.ipynb\">\n",
    "        <img src=\"https://i.ibb.co/xfJbPmL/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6873211b02d4"
   },
   "source": [
    "#### Import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-02T16:35:22.863599Z",
     "iopub.status.busy": "2020-10-02T16:35:22.863030Z",
     "iopub.status.idle": "2020-10-02T16:35:28.261974Z",
     "shell.execute_reply": "2020-10-02T16:35:28.261435Z"
    },
    "id": "71c626bbac35"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4041a2e9b310"
   },
   "source": [
    "## Simple Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98e0c38cf95d"
   },
   "source": [
    "There are three built-in RNN layers in Keras:\n",
    "\n",
    "1. [`keras.layers.SimpleRNN`](https://keras.io/api/layers/recurrent_layers/simple_rnn/), a fully-connected RNN where the output from previous\n",
    "timestep is to be fed to next timestep.\n",
    "\n",
    "```python\n",
    "tf.keras.layers.SimpleRNN(\n",
    "    units,\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    "    return_sequences=False,\n",
    "    return_state=False,\n",
    "    go_backwards=False,\n",
    "    stateful=False,\n",
    ")\n",
    "````\n",
    "\n",
    "2. [`keras.layers.GRU`](https://keras.io/api/layers/recurrent_layers/gru/), first proposed in\n",
    "[Cho et al., 2014](https://arxiv.org/abs/1406.1078).\n",
    "```python\n",
    "tf.keras.layers.GRU(\n",
    "    units,\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    "    return_sequences=False,\n",
    "    return_state=False,\n",
    "    go_backwards=False,\n",
    "    stateful=False,\n",
    ")\n",
    "```\n",
    "\n",
    "3. [`keras.layers.LSTM`](https://keras.io/api/layers/recurrent_layers/lstm/), first proposed in\n",
    "[Hochreiter & Schmidhuber, 1997](https://www.bioinf.jku.at/publications/older/2604.pdf).\n",
    "```python\n",
    "tf.keras.layers.LSTM(\n",
    "    units,\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    "    return_sequences=False,\n",
    "    return_state=False,\n",
    "    go_backwards=False,\n",
    "    stateful=False,\n",
    ")\n",
    "````\n",
    "For more information, see the\n",
    "[RNN API documentation](https://keras.io/api/layers/recurrent_layers/).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In time series forecasting we are going to use the **many-to-one** architecture with default parameter `return_sequences=False`.\n",
    "\n",
    "The shape of the output  for this architecture  is `(batch_size, units)`.\n",
    "where `units` corresponds to the `units` argument passed to the layer's constructor.\n",
    "\n",
    "Lets see one some examples for understanding the input/output dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-02T16:35:28.271135Z",
     "iopub.status.busy": "2020-10-02T16:35:28.270504Z",
     "iopub.status.idle": "2020-10-02T16:35:30.101802Z",
     "shell.execute_reply": "2020-10-02T16:35:30.102221Z"
    },
    "id": "a5617759e54e"
   },
   "outputs": [],
   "source": [
    "# dims of input: [batch, timesteps, features]\n",
    "inputs = tf.random.normal([32, 10, 4])\n",
    "print('input dim (batch, timesteps, feature): ', inputs.shape)\n",
    "# return_sequences=False, return_state=False\n",
    "lstm = tf.keras.layers.LSTM(units= 2)\n",
    "output = lstm(inputs)\n",
    "print('return_state=False output shape: ',output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep RNN\n",
    "We can stack multiple layers of RNNs on top of each other. Each hidden state is continuously passed to both the next time step of the current layer and the current time step of the next layer.\n",
    "\n",
    "For stack another RNN layer to an existing one, we need to use the states with `return_sequences=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can modify the input vector before the rnn cell with TimeDistributed\n",
    "timesteps = 10\n",
    "features = 8 # dimension of the innput of every cell\n",
    "\n",
    "#Shape [batch, timesteps, features] \n",
    "inputs = tf.keras.Input(shape=(timesteps, features), name='input')\n",
    "lstm_1 = layers.LSTM(64, return_sequences=True, name='lstm_1')(inputs)\n",
    "lstm_2 = layers.LSTM(64, return_sequences=True, name='lstm_2')(lstm_1)\n",
    "# last lstm layer depends in [one to many or  many to many]\n",
    "lstm_3 = layers.LSTM(64, return_sequences=False, name='lstm_3')(lstm_2)\n",
    "model = keras.Model(inputs=inputs, outputs=lstm_3, name='rnn_example')\n",
    "#print(model.summary())\n",
    "inputs = tf.random.normal([32, timesteps, features])\n",
    "print(model(inputs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNNs\n",
    "\n",
    "For sequences other than time series (e.g. text), it is often the case that a RNN model\n",
    "can perform better if it not only processes sequence from start to end, but also\n",
    "backwards. For example, to predict the next word in a sentence, it is often useful to\n",
    "have the context around the word, not only just the words that come before it.\n",
    "\n",
    "Keras provides an easy API for you to build such bidirectional RNNs: the\n",
    "`keras.layers.Bidirectional` wrapper.\n",
    "\n",
    "[link to documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "# If you crete a second layer you must set return_sequences=True\n",
    "model.add(\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=True), input_shape=(timesteps, features))\n",
    ")\n",
    "# Second Bidirectional layer\n",
    "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
    "# Output\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Windowing sequences\n",
    "\n",
    "<img src=\"https://i.ibb.co/5nvJQB4/split-window.png\" alt=\"cnn\" border=\"0\">\n",
    "\n",
    "Creates a dataset of sliding windows over a timeseries provided as array with [timeseries_dataset_from_array](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For creating the windows we use [timeseries_dataset_from_array](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array)\n",
    "\n",
    "```python\n",
    "tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "    data, targets, sequence_length, sequence_stride=1, sampling_rate=1,\n",
    "    batch_size=128, shuffle=False, seed=None, start_index=None, end_index=None\n",
    ")\n",
    "```\n",
    "\n",
    "For preprocessing the input we are going to use `prepare_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(ts, past, future, batch_size):\n",
    "    start = past + future - 1\n",
    "    x = ts[:-past + 1]\n",
    "    y = ts[start:start + len(x)]\n",
    "    # padding\n",
    "    y = np.concatenate([y, np.zeros(len(x) - len(y))])\n",
    "    dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "        x,\n",
    "        y,\n",
    "        sequence_length=past,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airlines Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('../data/airline-passengers.csv', usecols=[1])\n",
    "url = 'https://c4science.ch/diffusion/2096/browse/master/input/international-airline-passengers.csv'\n",
    "df = pd.read_csv(url, usecols=[1], nrows=144)\n",
    "dataset = df.values  # .flatten()\n",
    "plt.plot(dataset)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split in train/test set and scale with  StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_fraction = 0.75\n",
    "train_split = int(split_fraction * int(len(dataset)))\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# scale the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dataset[:train_split])\n",
    "\n",
    "\n",
    "dataset_s = scaler.fit_transform(dataset).flatten()\n",
    "plt.plot(dataset_s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "train, test = dataset_s[0:train_split], dataset_s[train_split:len(dataset)]\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lenght of sequence for predicting\n",
    "past = 2\n",
    "# future steps to predict\n",
    "future = 1\n",
    "\n",
    "batch_size = 4\n",
    "dataset_train = prepare_dataset(train, past, future,\n",
    "                    batch_size)\n",
    "dataset_val = prepare_dataset(test, past, future,\n",
    "                    batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for windows, targets in dataset_train:\n",
    "    print('windows shape: {}'.format(windows.shape))\n",
    "    print('targets shape: {}'.format(targets.shape))\n",
    "    for i in range(len(targets)):\n",
    "        print('window,target: {},{}'.format(windows[i,:].numpy(), targets[i].numpy()))   \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_shape = (past, 1)\n",
    "inputs = keras.layers.Input(shape=(past,1))\n",
    "lstm_out_1 = keras.layers.LSTM(32, return_sequences=False)(inputs)\n",
    "outputs = keras.layers.Dense(1)(lstm_out_1)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=\"mse\")\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=100,\n",
    "    validation_data=dataset_val\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(plot_data, delta, title):\n",
    "    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n",
    "    marker = [\".-\", \"rx\", \"go\"]\n",
    "    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n",
    "    if delta:\n",
    "        future = delta\n",
    "    else:\n",
    "        future = 0\n",
    "\n",
    "    plt.title(title)\n",
    "    for i, val in enumerate(plot_data):\n",
    "        if i:\n",
    "            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "    plt.legend()\n",
    "    plt.xlim([time_steps[0], (future + 5) * 2])\n",
    "    plt.xticks([])\n",
    "    plt.xlabel(\"Time-Step\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "for x, y in dataset_val.take(5):\n",
    "    x_in = scaler.inverse_transform(x.numpy())\n",
    "    y_in = scaler.inverse_transform(y.numpy())\n",
    "    pred = scaler.inverse_transform(model.predict(x))\n",
    "    show_plot(\n",
    "        [x_in[0,:], y_in[0], pred.flatten()[0]],\n",
    "        future,\n",
    "        \"Single Step Prediction\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# future steps to predict\n",
    "future = 1\n",
    "sequence_length = int(past)  # the that steps we use to predict\n",
    "\n",
    "x = dataset_s[:-past]\n",
    "# first target oof first window is len(window) + future\n",
    "y = dataset_s[past:]\n",
    "\n",
    "batch_size=200\n",
    "dataset_ts = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x, y, sequence_length=past, batch_size=batch_size)\n",
    "for x, y in dataset_ts:\n",
    "    x_in = scaler.inverse_transform(x.numpy())\n",
    "    y_in = scaler.inverse_transform(y.numpy())\n",
    "    pred = scaler.inverse_transform(model.predict(x))\n",
    "    \n",
    "plt.plot(y_in)\n",
    "plt.plot(pred)\n",
    "plt.legend(['Real','Prediction'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Climate Data Time-Series\n",
    "\n",
    "We will be using Jena Climate dataset recorded by the\n",
    "[Max Planck Institute for Biogeochemistry](https://www.bgc-jena.mpg.de/wetter/).\n",
    "The dataset consists of 14 features such as temperature, pressure, humidity etc, recorded once per\n",
    "10 minutes.\n",
    "\n",
    "**Location**: Weather Station, Max Planck Institute for Biogeochemistry\n",
    "in Jena, Germany\n",
    "\n",
    "**Time-frame Considered**: Jan 10, 2009 - December 31, 2016\n",
    "\n",
    "\n",
    "The table below shows the column names, their value formats, and their description.\n",
    "\n",
    "Index| Features      |Format             |Description\n",
    "-----|---------------|-------------------|-----------------------\n",
    "1    |Date Time      |01.01.2009 00:10:00|Date-time reference\n",
    "2    |p (mbar)       |996.52             |The pascal SI derived unit of pressure used to quantify internal pressure. Meteorological reports typically state atmospheric pressure in millibars.\n",
    "3    |T (degC)       |-8.02              |Temperature in Celsius\n",
    "4    |Tpot (K)       |265.4              |Temperature in Kelvin\n",
    "5    |Tdew (degC)    |-8.9               |Temperature in Celsius relative to humidity. Dew Point is a measure of the absolute amount of water in the air, the DP is the temperature at which the air cannot hold all the moisture in it and water condenses.\n",
    "6    |rh (%)         |93.3               |Relative Humidity is a measure of how saturated the air is with water vapor, the %RH determines the amount of water contained within collection objects.\n",
    "7    |VPmax (mbar)   |3.33               |Saturation vapor pressure\n",
    "8    |VPact (mbar)   |3.11               |Vapor pressure\n",
    "9    |VPdef (mbar)   |0.22               |Vapor pressure deficit\n",
    "10   |sh (g/kg)      |1.94               |Specific humidity\n",
    "11   |H2OC (mmol/mol)|3.12               |Water vapor concentration\n",
    "12   |rho (g/m ** 3) |1307.75            |Airtight\n",
    "13   |wv (m/s)       |1.03               |Wind speed\n",
    "14   |max. wv (m/s)  |1.75               |Maximum wind speed\n",
    "15   |wd (deg)       |152.3              |Wind direction in degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "\n",
    "uri = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\"\n",
    "zip_path = keras.utils.get_file(origin=uri, fname=\"jena_climate_2009_2016.csv.zip\")\n",
    "zip_file = ZipFile(zip_path)\n",
    "zip_file.extractall()\n",
    "csv_path = \"jena_climate_2009_2016.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path, parse_dates =['Date Time'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data Visualization\n",
    "\n",
    "To give us a sense of the data we are working with, each feature has been plotted below.\n",
    "This shows the distinct pattern of each feature over the time period from 2009 to 2016.\n",
    "It also shows where anomalies are present, which will be addressed during normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\n",
    "    \"Pressure\",\n",
    "    \"Temperature\",\n",
    "    \"Temperature in Kelvin\",\n",
    "    \"Temperature (dew point)\",\n",
    "    \"Relative Humidity\",\n",
    "    \"Saturation vapor pressure\",\n",
    "    \"Vapor pressure\",\n",
    "    \"Vapor pressure deficit\",\n",
    "    \"Specific humidity\",\n",
    "    \"Water vapor concentration\",\n",
    "    \"Airtight\",\n",
    "    \"Wind speed\",\n",
    "    \"Maximum wind speed\",\n",
    "    \"Wind direction in degrees\",\n",
    "]\n",
    "\n",
    "feature_keys = [\n",
    "    \"p (mbar)\",\n",
    "    \"T (degC)\",\n",
    "    \"Tpot (K)\",\n",
    "    \"Tdew (degC)\",\n",
    "    \"rh (%)\",\n",
    "    \"VPmax (mbar)\",\n",
    "    \"VPact (mbar)\",\n",
    "    \"VPdef (mbar)\",\n",
    "    \"sh (g/kg)\",\n",
    "    \"H2OC (mmol/mol)\",\n",
    "    \"rho (g/m**3)\",\n",
    "    \"wv (m/s)\",\n",
    "    \"max. wv (m/s)\",\n",
    "    \"wd (deg)\",\n",
    "]\n",
    "\n",
    "colors = [\n",
    "    \"blue\",\n",
    "    \"orange\",\n",
    "    \"green\",\n",
    "    \"red\",\n",
    "    \"purple\",\n",
    "    \"brown\",\n",
    "    \"pink\",\n",
    "    \"gray\",\n",
    "    \"olive\",\n",
    "    \"cyan\",\n",
    "]\n",
    "\n",
    "date_time_key = \"Date Time\"\n",
    "\n",
    "\n",
    "def show_raw_visualization(data):\n",
    "    time_data = data[date_time_key]\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=7, ncols=2, figsize=(15, 20), dpi=80, facecolor=\"w\", edgecolor=\"k\"\n",
    "    )\n",
    "    for i in range(len(feature_keys)):\n",
    "        key = feature_keys[i]\n",
    "        c = colors[i % (len(colors))]\n",
    "        t_data = data[key]\n",
    "        t_data.index = time_data\n",
    "        t_data.head()\n",
    "        ax = t_data.plot(\n",
    "            ax=axes[i // 2, i % 2],\n",
    "            color=c,\n",
    "            title=\"{} - {}\".format(titles[i], key),\n",
    "            rot=25,\n",
    "        )\n",
    "        ax.legend([titles[i]])\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "show_raw_visualization(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This heat map shows the correlation between different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_heatmap(data):\n",
    "    plt.matshow(data.corr())\n",
    "    plt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=90)\n",
    "    plt.gca().xaxis.tick_bottom()\n",
    "    plt.yticks(range(data.shape[1]), data.columns, fontsize=14)\n",
    "\n",
    "    cb = plt.colorbar()\n",
    "    cb.ax.tick_params(labelsize=14)\n",
    "    plt.title(\"Feature Correlation Heatmap\", fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_heatmap(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_title = ['Pressure', 'Temperature', 'Saturation vapor pressure',\n",
    "                 'Vapor pressure deficit', 'Specific' 'humidity', 'Airtight', 'Wind speed']\n",
    "selected_features = ['p (mbar)',\n",
    " 'T (degC)',\n",
    " 'VPmax (mbar)',\n",
    " 'VPdef (mbar)',\n",
    " 'sh (g/kg)',\n",
    " 'rho (g/m**3)',\n",
    " 'wv (m/s)',\n",
    "\"max. wv (m/s)\",\"wd (deg)\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Here we are picking ~300,000 data points for training. Observation is recorded every\n",
    "10 mins, that means 6 times per hour. We will resample one point per hour since no\n",
    "drastic change is expected within 60 minutes. We do this via the `sampling_rate`\n",
    "argument in `timeseries_dataset_from_array` utility.\n",
    "\n",
    "We are tracking data from past 720 timestamps (720/6=120 hours). This data will be\n",
    "used to predict the temperature after 72 timestamps (76/6=12 hours).\n",
    "\n",
    "Since every feature has values with\n",
    "varying ranges, we do normalization to confine feature values to a range of `[0, 1]` before\n",
    "training a neural network.\n",
    "We do this by subtracting the mean and dividing by the standard deviation of each feature.\n",
    "\n",
    "71.5 % of the data will be used to train the model, i.e. 300,693 rows. `split_fraction` can\n",
    "be changed to alter this percentage.\n",
    "\n",
    "The model is shown data for first 5 days i.e. 720 observations, that are sampled every\n",
    "hour. The temperature after 72 (12 hours * 6 observation per hour) observation will be\n",
    "used as a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.set_index('Date Time')\n",
    "## resample by the mean of hour values\n",
    "df =df.resample('1h').mean().fillna(method='ffill') # \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use only important features\n",
    "df_0 = df.copy()\n",
    "df = df[selected_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the correlation heatmap, few parameters like Relative Humidity and\n",
    "Specific Humidity are redundant. Hence we will be using select features, not all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train - test\n",
    "split_fraction = 0.7\n",
    "train_split = int(split_fraction * int(len(df)))\n",
    "\n",
    "train_data = df.iloc[0 : train_split]\n",
    "val_data = df.iloc[train_split:]\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df, past, future,\n",
    "                    batch_size, target):\n",
    "    start = past + future - 1\n",
    "    x = df.iloc[:-past + 1]\n",
    "    y = df.iloc[start:start + len(x)][target].values.flatten()\n",
    "    # padding\n",
    "    y = np.concatenate([y, np.zeros(len(x) - len(y))])\n",
    "    dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "        x,\n",
    "        y,\n",
    "        sequence_length=past,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `timeseries_dataset_from_array` function takes in a sequence of data-points gathered at\n",
    "equal intervals, along with time series parameters such as length of the\n",
    "sequences/windows, spacing between two sequence/windows, etc., to produce batches of\n",
    "sub-timeseries inputs and targets sampled from the main timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## previous hours to consider\n",
    "past = 6 \n",
    "## Number of hours later to predict\n",
    "future = 12\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "dataset_train = prepare_dataset(train_data, past, future,\n",
    "                    batch_size, target = 'T (degC)')\n",
    "dataset_val = prepare_dataset(val_data, past, future,\n",
    "                    batch_size, target= 'T (degC)')\n",
    "\n",
    "for batch in dataset_train.take(1):\n",
    "    inputs, targets = batch\n",
    "    break\n",
    "\n",
    "print(\"Input shape:\", inputs.numpy().shape)\n",
    "print(\"Target shape:\", targets.numpy().shape)\n",
    "inputs_shape = (inputs.shape[1], inputs.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the dataset for performance\n",
    "\n",
    "Let's make sure to use buffered prefetching so you can yield data from disk without having I/O become blocking. These are two important methods you should use when loading data.\n",
    "\n",
    "`Dataset.cache()` keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n",
    "\n",
    "`Dataset.prefetch()` overlaps data preprocessing and model execution while training. \n",
    "\n",
    "Interested readers can learn more about both methods, as well as how to cache data to disk in the [data performance guide](https://www.tensorflow.org/guide/data_performance#prefetching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUTOTUNE = tf.data.AUTOTUNE\n",
    "#dataset_train = dataset_train.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "#dataset_val = dataset_val.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization layer\n",
    "We can normalize the features with [Normalization layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Normalization).\n",
    "\n",
    "```python\n",
    "tf.keras.layers.experimental.preprocessing.Normalization(\n",
    "    axis=-1, dtype=None, mean=None, variance=None, **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "```python\n",
    "norm = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "norm.adapt(dataset_train.map(lambda x, y: x))\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "`adapt` computes mean and std of the train data and store them as the layer's weights. `adapt`\n",
    " should be called before fit, evaluate, or predict.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = tf.keras.layers.experimental.preprocessing.Normalization(\n",
    "    axis=-1, dtype=None, mean=None, variance=None\n",
    ")\n",
    "norm.adapt(dataset_train.map(lambda x, y: x))\n",
    "\n",
    "print('Unnormalized row: ', df.iloc[:1].values)\n",
    "print('Normalized row: ', norm(df.iloc[:1]))\n",
    "print('Normalized df, mean row: ',norm(df.values).numpy().mean(1))\n",
    "print('Normalized df, std row: ',np.std(norm(df.values).numpy(), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = tf.keras.layers.experimental.preprocessing.Normalization(\n",
    "    axis=-1\n",
    ")\n",
    "norm.adapt(dataset_train.map(lambda x, y: x))\n",
    "\n",
    "num_features =  9\n",
    "inputs_shape = (past, num_features)\n",
    "\n",
    "inputs = keras.layers.Input(shape=inputs_shape)\n",
    "inputs_norm = norm(inputs)\n",
    "rnn_out = keras.layers.SimpleRNN(32)(inputs_norm)\n",
    "outputs = keras.layers.Dense(1)(rnn_out)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `ModelCheckpoint` callback to regularly save checkpoints, and\n",
    "the `EarlyStopping` callback to interrupt training when the validation loss\n",
    "is not longer improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = \"model_checkpoint.h5\"\n",
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, patience=3)\n",
    "\n",
    "modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filepath=path_checkpoint,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=25,\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=[es_callback, modelckpt_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the loss with the function below. After one point, the loss stops\n",
    "decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(plot_data, delta, title):\n",
    "    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n",
    "    marker = [\".-\", \"rx\", \"go\"]\n",
    "    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n",
    "    if delta:\n",
    "        future = delta\n",
    "    else:\n",
    "        future = 0\n",
    "\n",
    "    plt.title(title)\n",
    "    for i, val in enumerate(plot_data):\n",
    "        if i:\n",
    "            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "    plt.legend()\n",
    "    plt.xlim([time_steps[0], (future + 5) * 2])\n",
    "    plt.xlabel(\"Time-Step\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "for x, y in dataset_val.take(5):\n",
    "    show_plot(\n",
    "        [x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "        future,\n",
    "        \"Single Step Prediction\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 500\n",
    "dataset_ts = prepare_dataset(val_data, past, future,\n",
    "                    batch_size, target = 'T (degC)')\n",
    "for i, (x, y) in enumerate(dataset_ts):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print('batch: ', i)\n",
    "    pred = model.predict(x)\n",
    "    plt.plot(y.numpy())\n",
    "    plt.plot(pred)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1:  Change the `keras.layers.SimpleRNN` layer to `keras.layers.LSTM` and to `keras.layers.GRU` and compare the results\n",
    "You can also set the `recurrent_dropout` parameter\n",
    "\n",
    "```python\n",
    "tf.keras.layers.x(\n",
    "    units,\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_features = ...\n",
    "inputs_shape = (past, num_features)\n",
    "\n",
    "\n",
    "norm = tf.keras.layers.experimental.preprocessing.Normalization(\n",
    "    axis=-1\n",
    ")\n",
    "norm.adapt(dataset_train.map(lambda x, y: x))\n",
    "\n",
    "inputs = layers.Input(shape=inputs_shape)\n",
    "inputs_norm = norm(inputs)\n",
    "\n",
    "## complete the code\n",
    "lstm_out = layers...(...)(...)\n",
    "outputs = layers.Dense(1)(lstm_out)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=\"mse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, patience=3)\n",
    "\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=25,\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=[es_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in dataset_val.take(5):\n",
    "    show_plot(\n",
    "        [x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "        future,\n",
    "        \"Single Step Prediction\",\n",
    "    )\n",
    "    \n",
    "    \n",
    "batch_size= 500\n",
    "dataset_ts = prepare_dataset(val_data, past, future,\n",
    "                    batch_size, target = 'T (degC)')\n",
    "for i, (x, y) in enumerate(dataset_ts):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print('batch: ', i)\n",
    "    pred = model.predict(x)\n",
    "    plt.plot(y.numpy())\n",
    "    plt.plot(pred)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Wind velocity\n",
    "One thing that should stand out is the min value of the wind velocity, wv (m/s) and max. wv (m/s) columns. This -9999 is likely erroneous. There's a separate wind direction column, so the velocity should be >=0. Replace it with zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = df['wv (m/s)']\n",
    "bad_wv = wv == -9999.0\n",
    "wv[bad_wv] = 0.0\n",
    "df['wv (m/s)'] = wv\n",
    "\n",
    "max_wv = df['max. wv (m/s)']\n",
    "bad_max_wv = max_wv == -9999.0\n",
    "max_wv[bad_max_wv] = 0.0\n",
    "df['max. wv (m/s)'] = max_wv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this will be easier for the model to interpret if you convert the wind direction and velocity columns to a wind vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = df.pop('wv (m/s)')\n",
    "\n",
    "# Convert to radians.\n",
    "wd_rad = df.pop('wd (deg)')*np.pi / 180\n",
    "\n",
    "# Calculate the wind x and y components.\n",
    "df['Wx'] = wv*np.cos(wd_rad)\n",
    "df['Wy'] = wv*np.sin(wd_rad)\n",
    "\n",
    "# Calculate the max wind x and y components.\n",
    "df['max Wx'] = max_wv*np.cos(wd_rad)\n",
    "df['max Wy'] = max_wv*np.sin(wd_rad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly the Date Time column is very useful, but not in this string form. Start by converting it to seconds:\n",
    "\n",
    "\n",
    "Similar to the wind direction the time in seconds is not a useful model input. Being weather data it has clear daily and yearly periodicity. There are many ways you could deal with periodicity.\n",
    "\n",
    "A simple approach to convert it to a usable signal is to use sin and cos to convert the time to clear \"Time of day\" and \"Time of year\" signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "timestamp_s = df.index.map(datetime.timestamp)\n",
    "day = 24*60*60\n",
    "year = (365.2425)*day\n",
    "\n",
    "df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train - test\n",
    "split_fraction = 0.7\n",
    "train_split = int(split_fraction * int(len(df)))\n",
    "\n",
    "train_data = df.iloc[0 : train_split]\n",
    "val_data = df.iloc[train_split:]\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = int(past / 1)\n",
    "\n",
    "batch_size = 128\n",
    "dataset_train = prepare_dataset(train_data, past, future,\n",
    "                    batch_size, target = 'T (degC)')\n",
    "dataset_val = prepare_dataset(val_data, past, future,\n",
    "                    batch_size, target= 'T (degC)')\n",
    "\n",
    "for batch in dataset_train.take(1):\n",
    "    inputs, targets = batch\n",
    "\n",
    "print(\"Input shape:\", inputs.numpy().shape)\n",
    "print(\"Target shape:\", targets.numpy().shape)\n",
    "inputs_shape = (inputs.shape[1], inputs.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "dataset_train = dataset_train.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "dataset_val = dataset_val.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:  Use the same model as before and compare the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "norm = tf.keras.layers.experimental.preprocessing.Normalization(\n",
    "    axis=-1\n",
    ")\n",
    "norm.adapt(dataset_train.map(lambda x, y: x))\n",
    "\n",
    "num_features = ...\n",
    "inputs_shape = (past, num_features)\n",
    "\n",
    "inputs = layers.Input(shape=inputs_shape)\n",
    "inputs_norm = norm(inputs)\n",
    "## complete the code\n",
    "lstm_out = layers..(...)(...)\n",
    "outputs = layers.Dense(1)(lstm_out)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=\"mse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, patience=3)\n",
    "\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=25,\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=[es_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in dataset_val.take(5):\n",
    "    show_plot(\n",
    "        [x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "        future,\n",
    "        \"Single Step Prediction\",\n",
    "    )\n",
    "    \n",
    "    \n",
    "batch_size= 500\n",
    "dataset_ts = prepare_dataset(val_data, past, future,\n",
    "                    batch_size, target = 'T (degC)')\n",
    "for i, (x, y) in enumerate(dataset_ts):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print('batch: ', i)\n",
    "    pred = model.predict(x)\n",
    "    plt.plot(y.numpy())\n",
    "    plt.plot(pred)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:  Create a deep model stacking two recurrent layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_features = ...\n",
    "inputs_shape = (past, num_features)\n",
    "\n",
    "\n",
    "norm = tf.keras.layers.experimental.preprocessing.Normalization(\n",
    "    axis=-1\n",
    ")\n",
    "norm.adapt(dataset_train.map(lambda x, y: x))\n",
    "\n",
    "inputs = layers.Input(shape=inputs_shape)\n",
    "inputs_norm = norm(inputs)\n",
    "\n",
    "## complete the code\n",
    "l_1 = keras.layers.LSTM(..., return_sequences=...)(...)\n",
    "l_2 = keras.layers.LSTM(..., return_sequences=...)(...)\n",
    "\n",
    "outputs = keras.layers.Dense(1)(...)\n",
    "\n",
    "\n",
    "model = keras.Model(inputs=...)\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=\"mse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, patience=3)\n",
    "\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=25,\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=[es_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in dataset_val.take(5):\n",
    "    show_plot(\n",
    "        [x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "        future,\n",
    "        \"Single Step Prediction\",\n",
    "    )\n",
    "    \n",
    "    \n",
    "batch_size= 500\n",
    "dataset_ts = prepare_dataset(val_data, past, future,\n",
    "                    batch_size, target = 'T (degC)')\n",
    "for i, (x, y) in enumerate(dataset_ts):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print('batch: ', i)\n",
    "    pred = model.predict(x)\n",
    "    plt.plot(y.numpy())\n",
    "    plt.plot(pred)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4:  Use `Bidirectional`  layer\n",
    "\n",
    "```python\n",
    "layers.Bidirectional(layers.LSTM(64, return_sequences=)\n",
    "````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "norm = tf.keras.layers.experimental.preprocessing.Normalization(\n",
    "    axis=-1\n",
    ")\n",
    "norm.adapt(dataset_train.map(lambda x, y: x))\n",
    "\n",
    "\n",
    "inputs = layers.Input(shape=(..., ...))\n",
    "inputs_norm = norm(inputs)\n",
    "\n",
    "## complete the code\n",
    "l_1 = ...(...)\n",
    "\n",
    "outputs = keras.layers.Dense(1)(...)\n",
    "\n",
    "\n",
    "model = keras.Model(inputs=...)\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=\"mse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, patience=3)\n",
    "\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=25,\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=[es_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in dataset_val.take(5):\n",
    "    show_plot(\n",
    "        [x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "        future,\n",
    "        \"Single Step Prediction\",\n",
    "    )\n",
    "    \n",
    "    \n",
    "batch_size= 500\n",
    "dataset_ts = prepare_dataset(val_data, past, future,\n",
    "                    batch_size, target = 'T (degC)')\n",
    "for i, (x, y) in enumerate(dataset_ts):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print('batch: ', i)\n",
    "    pred = model.predict(x)\n",
    "    plt.plot(y.numpy())\n",
    "    plt.plot(pred)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5:  Obtain a good model for predicting the temperature in 24h\n",
    "\n",
    "Try different architectures and different values for `past`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past = ...\n",
    "batch_size = ...\n",
    "dataset_train = prepare_dataset(train_data, past, future,\n",
    "                    batch_size, target = 'T (degC)')\n",
    "dataset_val = prepare_dataset(val_data, past, future,\n",
    "                    batch_size, target= 'T (degC)')\n",
    "\n",
    "for batch in dataset_train.take(1):\n",
    "    inputs, targets = batch\n",
    "\n",
    "print(\"Input shape:\", inputs.numpy().shape)\n",
    "print(\"Target shape:\", targets.numpy().shape)\n",
    "inputs_shape = (inputs.shape[1], inputs.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# dataset_train = dataset_train.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "# dataset_val = dataset_val.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = tf.keras.layers.experimental.preprocessing.Normalization(\n",
    "    axis=-1\n",
    ")\n",
    "norm.adapt(dataset_train.map(lambda x, y: x))\n",
    "\n",
    "\n",
    "inputs = layers.Input(shape=(..., ...))\n",
    "inputs_norm = norm(inputs)\n",
    "\n",
    "\n",
    "l_1 = keras.layers.LSTM(..., return_sequences=...)(...)\n",
    "l_2 = keras.layers.LSTM(..., return_sequences=...)(...)\n",
    "\n",
    "outputs = keras.layers.Dense(1)(...)\n",
    "\n",
    "model = keras.Model(inputs=...)\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = \"model_checkpoint.h5\"\n",
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, patience=3)\n",
    "\n",
    "modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filepath=path_checkpoint,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=25,\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=[es_callback, modelckpt_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in dataset_val.take(5):\n",
    "    show_plot(\n",
    "        [x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "        future,\n",
    "        \"Single Step Prediction\",\n",
    "    )\n",
    "    \n",
    "    \n",
    "batch_size= 500\n",
    "dataset_ts = prepare_dataset(val_data, past, future,\n",
    "                    batch_size, target = 'T (degC)')\n",
    "for i, (x, y) in enumerate(dataset_ts):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print('batch: ', i)\n",
    "    pred = model.predict(x)\n",
    "    plt.plot(y.numpy())\n",
    "    plt.plot(pred)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice: Cryptocurrency Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, obtain and pre-process the data from [www.cryptodatadownload.com](www.cryptodatadownload.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.cryptodatadownload.com/cdd/gemini_{0}USD_1hr.csv\n",
    "# '../data/gemini_{0}USD_1hr.csv'\n",
    "def get_coin_df(fpath='../data/gemini_{0}USD_1hr.csv'):\n",
    "    coins = ['BTC', 'ETH', 'LTC', 'ZEC']\n",
    "    coins = ['BTC', 'ETH']\n",
    "    file_path = fpath.format('BTC')\n",
    "    print(file_path)\n",
    "    df = pd.read_csv(file_path, parse_dates=['Date'], skiprows=1, skipfooter=1)\n",
    "    df = df[['Date', 'Open']]\n",
    "    df = df.set_index('Date')\n",
    "    df = df.sort_index()\n",
    "    df = df.rename(columns={'Open': 'BTC'})\n",
    "    for coin in ['ETH', 'LTC', 'ZEC']:\n",
    "        coin_path = fpath.format(coin)\n",
    "        df_coin = pd.read_csv(coin_path,\n",
    "                              parse_dates=['Date'],\n",
    "                              skiprows=1,\n",
    "                              skipfooter=1)\n",
    "        df_coin = df_coin[['Date', 'Open']]\n",
    "        df_coin = df_coin.set_index('Date')\n",
    "        df_coin = df_coin.sort_index()\n",
    "        df_coin = df_coin.rename(columns={'Open': coin})\n",
    "        df[coin] = np.nan\n",
    "        df.loc[df_coin.index, coin] = df_coin.values.flatten()\n",
    "    return df\n",
    "\n",
    "\n",
    "df = get_coin_df(\n",
    "    fpath='http://www.cryptodatadownload.com/cdd/gemini_{0}USD_1hr.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.diff()\n",
    "# df[df.columns] = np.sqrt(df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the scaled values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Create an object to transform the data to fit minmax processor\n",
    "x_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Run the normalizer on the dataframe\n",
    "df_normalized = pd.DataFrame(x_scaled, columns=df.columns, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coins = ['BTC', 'ETH', 'LTC', 'ZEC']\n",
    "for col in coins:\n",
    "    df_normalized[col].plot(legend=True, figsize=(15, 7))\n",
    "plt.ylabel('Dolars scaled')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Time Series\n",
    "To start with, we will use only one cryptocurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_target = 'ETH' #  Coin that we want to predict, ['BTC', 'ETH', 'LTC', 'ZEC']\n",
    "df_coin = df[coin_target].copy()\n",
    "# delete nan rows\n",
    "df_coin = df_coin.dropna()\n",
    "df_coin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coin.plot(legend=True,figsize=(10,5))\n",
    "plt.ylabel(coin_target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "test_date = pd.Timestamp(\"2021-04-10\")\n",
    "init_date = pd.Timestamp(\"2017-10-08 14:00:00\")\n",
    "\n",
    "# train_data = df_coin.loc[df_coin.index < test_date].values\n",
    "\n",
    "train_data = df_coin.loc[(df_coin.index < test_date) *\n",
    "                         (df_coin.index > init_date)].values\n",
    "\n",
    "test_data = df_coin.loc[df_coin.index >= test_date].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = train_data.mean()\n",
    "sigma = train_data.std()\n",
    "print('mu, sigma: ', mu, sigma)\n",
    "\n",
    "train_data = (train_data - mu) / sigma\n",
    "test_data = (test_data - mu) / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform input array to time series matrix\n",
    "\n",
    "For example if  `dataset = [0,1,2,3,4,5,6]` :\n",
    "\n",
    "If we have `past=3`, and `future=2`, we use **windows of size 3** for predicting **2 steps ahead**.\n",
    "\n",
    "we are going to use `[0,1,2] (length=past=3)` to predict `4`, \n",
    "\n",
    "We need too create a training data like\n",
    "```python\n",
    "[0,1,2], 4 \n",
    "[1,2,3], 5 \n",
    "[2,3,4], 6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def convert2matrix(data_arr, past, future, shuffle=False):\n",
    "    X, Y = [], []\n",
    "    size = len(data_arr)\n",
    "    for i in range(size - future - past + 1):\n",
    "        d = i + past\n",
    "        y_ind = i + past + future - 1\n",
    "        X.append(data_arr[i:d])\n",
    "        Y.append(data_arr[y_ind])\n",
    "    if shuffle:\n",
    "        c = list(zip(X, Y))\n",
    "        random.shuffle(c)\n",
    "        X, Y = zip(*c)\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the function works, we should obtain:\n",
    "```python\n",
    "[0,1,2], 4 \n",
    "[1,2,3], 5 \n",
    "[2,3,4], 6\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trial, y_trial = convert2matrix(np.array([0, 1, 2, 3, 4, 5, 6]),\n",
    "                                  past=3,\n",
    "                                  future=2,\n",
    "                                  shuffle=False)\n",
    "for ind in range(len(y_trial)):\n",
    "    print(X_trial[ind, :], y_trial[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Create a model to predict the ETH value in 24h, `RMSE(test) < 50$`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create windows\n",
    "past, future = (6, 24)\n",
    "X_train, y_train = convert2matrix(train_data, past, future, shuffle=True)\n",
    "X_test, y_test = convert2matrix(test_data, past, future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0, :], y_test[0], test_data[:past + future]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "## Modify the initial model to obtain better results\n",
    "\n",
    "inputs = keras.layers.Input(shape=(past, 1))\n",
    "\n",
    "l_1 = layers.SimpleRNN(1, return_sequences=False) (inputs) \n",
    "outputs = layers.Dense(1)(l_1)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=\"mse\")\n",
    "model.summary()\n",
    "\n",
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, patience=8)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    validation_split=0.2, shuffle=True, batch_size = 64, callbacks=[es_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test).flatten() * sigma + mu\n",
    "y_target = y_test * sigma + mu\n",
    "diff = y_pred - y_target\n",
    "print('max deviation: ', np.abs(y_pred - y_target).max())\n",
    "print('RMSE: ', np.mean((y_pred - y_target)**2)**0.5)\n",
    "print('MAE: ', np.abs(y_pred - y_target).mean())\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(y_target)\n",
    "plt.plot(y_pred)\n",
    "plt.legend(['True {0}'.format(coin_target), 'Predictions {0}'.format(coin_target)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can try feature engineering with df\n",
    "coin_target = 'ETH' # 'BTC'\n",
    "coins =  ['ETH', 'LTC', 'ZEC', 'BTC'] # ['BTC', 'ETH', 'LTC', 'ZEC']\n",
    "df_multi = df[coins].dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can create new features\n",
    "\n",
    "## moving average features\n",
    "for coin in coins:\n",
    "    df_multi[coin+'_week_mu'] = df_multi[coin].rolling(window=7*24).mean()\n",
    "    df_multi[coin+'_week_sigma'] = df_multi[coin].rolling(window=7*24).std()\n",
    "df_multi = df_multi.dropna()\n",
    "\n",
    "## time features\n",
    "dt = df_multi.index.to_numpy()\n",
    "dt = (dt - dt.min()) / np.timedelta64(24 * 365, 'h')\n",
    "df_multi['time'] = dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2matrix_multi(df, past, future, target, shuffle=False):\n",
    "    X, Y = [], []\n",
    "    size = len(df)\n",
    "    for i in range(size - future - past + 1):\n",
    "        d = i + past\n",
    "        y_ind = i + past + future - 1\n",
    "        X.append(df.iloc[i:d, :].values)\n",
    "        Y.append(df.iloc[y_ind][target])\n",
    "    if shuffle:\n",
    "        c = list(zip(X, Y))\n",
    "        random.shuffle(c)\n",
    "        X, Y = zip(*c)\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "#train_data = df_multi.loc[df_multi.index < test_date, :]\n",
    "train_data = df_multi.loc[(df_multi.index < test_date) * (df_multi.index > init_date ), : ].copy()\n",
    "test_data = df_multi.loc[df_multi.index >= test_date, :].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_multi.columns\n",
    "mu_dict = {}\n",
    "sigma_dict = {}\n",
    "for c in features:\n",
    "    mu = train_data[c].mean()\n",
    "    sigma = train_data[c].std()\n",
    "    mu_dict[c] = mu\n",
    "    sigma_dict[c] = sigma\n",
    "    print('feature: {0} , mu,sigma'.format(c), mu, sigma)\n",
    "print(mu_dict, sigma_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in features:\n",
    "    mu = mu_dict[c]\n",
    "    sigma = sigma_dict[c]\n",
    "    train_data.loc[:, c] = (train_data[c].values - mu) / sigma\n",
    "    test_data.loc[:, c] = (test_data[c].values - mu) / sigma\n",
    "train_data.describe()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Create a model to predict the ETH value in 24h, `RMSE(test) < 50$`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create windows\n",
    "past, future = (..., 24)\n",
    "X_train, y_train = convert2matrix_multi(\n",
    "    train_data, past, future, target=coin_target, shuffle=True)\n",
    "X_test, y_test = convert2matrix_multi(\n",
    "    test_data, past, future, target=coin_target, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.layers.Input(shape=(past, len(features)))\n",
    "...\n",
    "outputs = layers.Dense(1)(...)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=\"mse\")\n",
    "\n",
    "model.summary()\n",
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, patience=8)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    validation_split=0.2, shuffle=True, batch_size = 64, callbacks=[es_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = sigma_dict[coin_target]\n",
    "mu = mu_dict[coin_target]\n",
    "y_pred = model.predict(X_test).flatten() * sigma + mu\n",
    "y_target = y_test * sigma + mu\n",
    "diff = y_pred - y_target\n",
    "print('max deviation: ', np.abs(y_pred - y_target).max())\n",
    "print('RMSE: ', np.mean((y_pred - y_target)**2)**0.5)\n",
    "print('MAE: ', np.abs(y_pred - y_target).mean())\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(y_target)\n",
    "plt.plot(y_pred)\n",
    "plt.legend(['True {0}'.format(coin_target), 'Predictions {0}'.format(coin_target)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://www.tensorflow.org/guide/keras/rnn\n",
    "\n",
    "https://keras.io/examples/timeseries/timeseries_weather_forecasting/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
