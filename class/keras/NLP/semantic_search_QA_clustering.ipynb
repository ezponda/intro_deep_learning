{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "540c8652",
   "metadata": {},
   "source": [
    "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "<table align=\"center\">\n",
    " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/NLP/semantic_search_QA_clustering.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/NLP/semantic_search_QA_clustering.ipynb\">\n",
    "        <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e220629",
   "metadata": {},
   "source": [
    "# Semantic search & QA\n",
    "\n",
    "In this notebook, we'll introduce semantic search and question-answering using [`sentence-transformers`](https://www.sbert.net/), a Python library for state-of-the-art sentence, text and image embeddings. These embeddings are useful for semantic similarity tasks, such as information retrieval and question-answering systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85446b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the sentence-transformers library\n",
    "#!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef53ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import gzip\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee4a0a8",
   "metadata": {},
   "source": [
    "We'll use a pre-trained Sentence Transformer model to generate sentence embeddings. Many pre-trained models are available [here](https://www.sbert.net/docs/pretrained_models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56081ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ab530d",
   "metadata": {},
   "source": [
    "For our semantic search and question-answering task, we need a list of documents or paragraphs to search through for relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b473c53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample paragraphs\n",
    "paragraphs = [\n",
    "    \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\",\n",
    "    \"The Statue of Liberty is a colossal neoclassical sculpture on Liberty Island in New York Harbor within New York City, in the United States.\",\n",
    "    \"The Great Wall of China is a series of fortifications made of stone, brick, tamped earth, wood, and other materials, generally built along an east-to-west line across the historical northern borders of China.\",\n",
    "    \"The Colosseum, also known as the Flavian Amphitheatre, is an oval amphitheatre in the centre of the city of Rome, Italy.\",\n",
    "    \"The Taj Mahal is an ivory-white marble mausoleum on the southern bank of the river Yamuna in the Indian city of Agra.\"\n",
    "]\n",
    "\n",
    "paragraphs = np.array(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad2ca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for paragraphs\n",
    "corpus_embeddings = model.encode(paragraphs)\n",
    "print(corpus_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a5756f",
   "metadata": {},
   "source": [
    "Now, let's define a function to perform semantic search, given a query and a list of paragraph embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a9592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, model, corpus_embeddings, paragraphs, top_k=2):\n",
    "    query_embedding = model.encode([query])[0]\n",
    "    similarities = cosine_similarity([query_embedding], corpus_embeddings)[0]\n",
    "    indexes = np.argpartition(similarities, -top_k)[-top_k:]\n",
    "    indexes = indexes[np.argsort(-similarities[indexes])]\n",
    "    print(f\"Input query: {query}\")\n",
    "    print()\n",
    "    for text, sim in zip(list(paragraphs[indexes]), similarities[indexes].tolist()):\n",
    "        print(f\"{sim:.3f}\\t{text}\")\n",
    "              \n",
    "\n",
    "semantic_search('Where is the Colosseum', model, corpus_embeddings, paragraphs, top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df5645",
   "metadata": {},
   "source": [
    "## Multilingual models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b0cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try in other languages\n",
    "semantic_search('¿Dónde está el Coliseo?', model, corpus_embeddings, paragraphs, top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d55f723",
   "metadata": {},
   "source": [
    "We have multilinguals models available [here](https://www.sbert.net/docs/pretrained_models.html#multi-lingual-models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea38223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use multilingual models \n",
    "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "multi_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7e03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_corpus_embeddings = multi_model.encode(paragraphs)\n",
    "print(multi_corpus_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d89f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_search('¿Dónde está el Coliseo?', multi_model, multi_corpus_embeddings, paragraphs, top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08858a4b",
   "metadata": {},
   "source": [
    "## Wikipedia semantic search\n",
    "\n",
    "As dataset, we use Simple English Wikipedia. Compared to the full English wikipedia, it has only\n",
    "about 170k articles. We split these articles into paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aef3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_filepath = 'data/simplewiki-2020-11-01.jsonl.gz'\n",
    "\n",
    "if not os.path.exists(wikipedia_filepath):\n",
    "    util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath)\n",
    "\n",
    "passages = []\n",
    "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        data = json.loads(line.strip())\n",
    "        for paragraph in data['paragraphs']:\n",
    "            # We encode the passages as [title, text]\n",
    "            passages.append(data['title']+':  '+ paragraph)\n",
    "\n",
    "# If you like, you can also limit the number of passages you want to use\n",
    "print(\"Passages:\", len(passages))\n",
    "print(passages[0])\n",
    "print(passages[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa111507",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_passages = np.array(passages[:5000])\n",
    "reduced_passages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4cc331",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embeddings = model.encode(reduced_passages, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d1260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_search('Best american actor', model, corpus_embeddings, reduced_passages, top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad4f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_search('Number countries Europe', model, corpus_embeddings, reduced_passages, top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327dce75",
   "metadata": {},
   "source": [
    "### Question1: Load a different pre-trained Sentence Transformer model and compare its performance to the last model on the same set of paragraphs and queries. Which model performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043b5d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a different pre-trained model, generate embeddings, and test with the same queries\n",
    "model_name = ...\n",
    "new_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb3ca16",
   "metadata": {},
   "source": [
    "## Question 2: Find text duplicates\n",
    "\n",
    "Try to find duplicate or near-duplicate texts in a given corpus based on their semantic similarity using sentence-transformers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e63124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The quick brown fox leaps over the lazy dog.\",\n",
    "    \"The sky is blue, and the grass is green.\",\n",
    "    \"The grass is green, and the sky is blue.\",\n",
    "    \"It's a sunny day today.\",\n",
    "    \"The weather is sunny today.\",\n",
    "    \"She was wearing a beautiful red dress.\",\n",
    "    \"She had on a gorgeous red dress.\",\n",
    "    \"I'm going to the supermarket to buy some groceries.\",\n",
    "    \"I'm heading to the supermarket to purchase some groceries.\",\n",
    "    \"He didn't like the movie because it was too long.\",\n",
    "    \"He disliked the movie as it was too lengthy.\",\n",
    "    \"The train was delayed due to technical issues.\",\n",
    "    \"Technical issues caused the train to be delayed.\",\n",
    "    \"I'll have a cup of coffee with milk and sugar, please.\",\n",
    "    \"Can I get a coffee with milk and sugar, please?\",\n",
    "    \"The conference was very informative and interesting.\",\n",
    "    \"The conference turned out to be interesting and informative.\",\n",
    "    \"He enjoys listening to classical music in his free time.\",\n",
    "    \"In his leisure time, he likes to listen to classical music.\",\n",
    "    \"Please make sure you turn off the lights before leaving.\",\n",
    "    \"Before leaving, ensure that you switch off the lights.\",\n",
    "    \"The boy was delighted with the gift he received.\",\n",
    "    \"Receiving the present made the young lad ecstatic.\",\n",
    "    \"She has a preference for Italian cuisine.\",\n",
    "    \"Her favorite type of food is from Italy.\",\n",
    "    \"The software engineer resolved the issue by modifying the code.\",\n",
    "    \"By altering the programming, the tech expert fixed the problem.\",\n",
    "    \"Due to the inclement weather, the baseball game was postponed.\",\n",
    "    \"The baseball match was rescheduled because of bad weather conditions.\",\n",
    "    \"The house was engulfed in a raging fire.\",\n",
    "    \"Flames rapidly consumed the residence.\",\n",
    "    \"He is constantly browsing the internet for the latest news.\",\n",
    "    \"He frequently scours the web to stay updated on current events.\",\n",
    "    \"The puppy was playing with a toy in the garden.\",\n",
    "    \"In the yard, the young dog was frolicking with its plaything.\",\n",
    "    \"The artist painted a beautiful landscape on the canvas.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b500687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize the SentenceTransformer model\n",
    "model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe93930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Obtain corpus embeddings\n",
    "embeddings = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce3fd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Calculate similarity and find duplicates\n",
    "\n",
    "# TODO: Define a similarity threshold\n",
    "similarity_threshold = ...\n",
    "\n",
    "# TODO: Iterate over each pair of embeddings in the corpus\n",
    "# Calculate the cosine similarity between the embeddings\n",
    "# If the similarity is above the threshold, add the sentences to the duplicates list\n",
    "duplicates = []\n",
    "\n",
    "for i, emb1 in enumerate(embeddings):\n",
    "    for j, emb2 in enumerate(embeddings[i + 1:]):\n",
    "        similarity = cosine_similarity([emb1], [emb2])[0][0]\n",
    "        if ...:\n",
    "            duplicates.append((corpus[i], corpus[i + j + 1], similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b17148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicate sentences:\")\n",
    "for sent1, sent2, sim in duplicates:\n",
    "    print(f\"{sent1} | {sent2} | Similarity: {sim:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dbab82",
   "metadata": {},
   "source": [
    "# Document Clustering\n",
    "\n",
    "K-means clustering is a popular unsupervised machine learning algorithm that groups data points into k clusters based on their similarity. In our case, we want to group documents based on their semantic similarity. The algorithm requires us to specify the number of clusters k in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c055ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The apple is a sweet fruit\",\n",
    "    \"Oranges are citrus fruits\",\n",
    "    \"Bananas are rich in potassium\",\n",
    "    \"Strawberries are red fruits\",\n",
    "    \"Dogs are domesticated animals\",\n",
    "    \"Cats are also pets\",\n",
    "    \"Elephants are the largest land mammals\",\n",
    "    \"Cows provide us with milk\",\n",
    "    \"Sharks are marine predators\",\n",
    "    \"Whales are the largest marine mammals\",\n",
    "    \"Dolphins are very intelligent\",\n",
    "    \"Artificial intelligence is the future\",\n",
    "    \"Machine learning is a subset of AI\",\n",
    "    \"Deep learning is a part of machine learning\",\n",
    "    \"Neural networks are used in deep learning\",\n",
    "]\n",
    "\n",
    "df = pd.DataFrame({'documents': corpus})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07529ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode the documents in the corpus\n",
    "document_embeddings = model.encode(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea806d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 3\n",
    "clustering_model = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=300, n_init=10)\n",
    "clustering_model.fit(document_embeddings)\n",
    "cluster_assignment = clustering_model.labels_\n",
    "\n",
    "df['cluster'] = cluster_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc24cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_clusters):\n",
    "    print(f\"Cluster {i}:\")\n",
    "    print(df[df['cluster'] == i]['documents'].values, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aadaa3a",
   "metadata": {},
   "source": [
    "# Community Detection with Sentence Transformers\n",
    "\n",
    "The sentence_transformers library provides a utility for community detection which applies a threshold on the cosine similarity score to identify distinct communities of sentences that are semantically similar. This method can be particularly helpful for organizing a large corpus of text into meaningful groups.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The [`community_detection`](https://www.sbert.net/docs/package_reference/util.html#sentence_transformers.util.community_detection) function in the Sentence Transformers library is a useful utility for finding clusters or communities of semantically similar sentences. Here are the details of the function parameters:\n",
    "\n",
    "- `document_embeddings`: This is the list of embeddings for the documents in your corpus. The embeddings can be created using any of the Sentence Transformer models. The embeddings should be in the form of a 2D tensor or a list of 1D tensors. Each embedding should be a fixed-length vector that represents the semantic meaning of a document.\n",
    "\n",
    "- `threshold`: This is a float value between 0 and 1 that determines the cutoff for considering two documents to be part of the same community. It's based on the cosine similarity of the document embeddings. If the cosine similarity of two document embeddings is greater than the threshold, those two documents are considered to be in the same community. The higher the threshold, the more similar the documents in each community will be. However, a higher threshold may also result in more communities.\n",
    "\n",
    "- `min_community_size`: This is the minimum number of documents that a community must have. If a community has fewer than this number of documents, it will be discarded. The default value is 1, but you might want to set a higher value if you're interested in larger communities. This can help filter out noise and find more meaningful communities.\n",
    "\n",
    "- `batch_size`: As the function computes cosine similarities between document pairs, it may consume a significant amount of memory for a large corpus. To manage this, the computations are done in batches. The batch_size parameter determines the number of document pairs to compute similarities for in each batch. Larger batch sizes can be faster but consume more memory, while smaller batch sizes are slower but more memory-efficient.\n",
    "\n",
    "The function returns a list of communities, where each community is a list of indices in the original list of documents. Each community represents a group of semantically similar documents based on the provided threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3d1c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.util import community_detection\n",
    "document_embeddings = model.encode(\n",
    "    corpus, show_progress_bar=True, convert_to_tensor=True\n",
    ")\n",
    "communities = community_detection(\n",
    "    document_embeddings, threshold=0.5, min_community_size=2, batch_size=1024\n",
    ")\n",
    "for i, comm in enumerate(communities):\n",
    "    print('_'*50)\n",
    "    print(f'community: {i}, size: {len(comm)}')\n",
    "    print('\\n'.join([corpus[ind] for ind in comm]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b87688",
   "metadata": {},
   "source": [
    "In the output, we will see the communities of semantically similar sentences. Note that the choice of the threshold value can greatly affect the results: a lower threshold will result in larger but less cohesive communities, while a higher threshold will result in smaller but more tightly-knit communities.\n",
    "\n",
    "The community_detection function is a fast and efficient way to group similar sentences together, but keep in mind that it's a rather simple method based on thresholding the cosine similarity, and more sophisticated community detection methods might yield better results for certain tasks or datasets.\n",
    "\n",
    "This function is a great way to explore the semantic structure of your corpus and to get a high-level understanding of the main themes or topics in your text data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
